{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 7: Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FAFAD2;color:#8B0000;border:1px solid #DCDCDC;padding: 5px;\">\n",
    "<h3>Things for you to do</h3>\n",
    "<ul>\n",
    "<li><span style=\"color:#8B0000\">The first thing you need to do is run the following cell. This will give you access to the Sussex NLTK package.</span></li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'T:\\Departments\\Informatics\\LanguageEngineering')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This session concerns the task of Dependency Parsing. You will be using our Python implementation of [arc-eager transition-based dependency parsing](http://www.aclweb.org/anthology/J08-4003). The aim of this session is to get you comfortable with reading dependency trees, and learning about new dependency relations.\n",
    "\n",
    "The focus will initially be on the direct object relation \"dobj\". You will use it to find what people are wanting, loving, and buying. Your aim by the end of the lab session should be to understand this relation and be able to spot when the parser gets it wrong or right.\n",
    "\n",
    "You will be given a dependency parser that has been pre-trained on the Wall Street Journal (which uses [stanford dependency relations}(http://nlp.stanford.edu/software/dependencies_manual.pdf) and the [same PoS tags](http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) as the NLTK PoS tagger).\n",
    "\n",
    "Dependency parsing produces a dependency tree view of a sentence, allowing us to see how the words function with each other, rather than just viewing the sentence as an unordered bag of words.\n",
    "\n",
    "Notice that not only must sentences be tokenised before being passed to the parser, they must also be Part-of-Speech (PoS) tagged. The parser relies heavily on PoS information to learn and make decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing a sentence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by looking at how to run the dependency parser on a list of sentences that you create."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FAFAD2;color:#8B0000;border:1px solid #DCDCDC;padding: 5px;\"> \n",
    "<h3>Things for you to do</h3>\n",
    "<ul>\n",
    "<li>Make a short list of example sentences to experiment with.  \n",
    "For example\n",
    "<code style=\"background-color: #F5F5F5;color:#8B0000\">sentences = [\"This is a great product!\", \"I really wish I hadn't bought this.\"]</code></li>\n",
    "<li>Use nltk tokeniser, <code style=\"background-color: #F5F5F5;color:#8B0000\">word_tokenize</code>, to tokenise your sentences to produce a list of tokenised sentences.  \n",
    "Call this list <code style=\"background-color: #F5F5F5;color:#8B0000\">tokenised_sentences</code>.  \n",
    "You will need to <code style=\"background-color: #F5F5F5;color:#8B0000\">import word_tokenize</code> from <code style=\"background-color: #F5F5F5;color:#8B0000\">nltk.tokenize</code>.</li>\n",
    "<li>Use the nltk Part-of-Speech tagger, <code style=\"background-color: #F5F5F5;color:#8B0000\">pos_tag</code>, to PoS tag your tokenised sentences to produce a list of PoS tagged sentences.\n",
    "Call this list <code style=\"background-color: #F5F5F5;color:#8B0000\">tagged_sentences</code>.   \n",
    "You will need to <code style=\"background-color: #F5F5F5;color:#8B0000\">import pos_tag</code> from <code style=\"background-color: #F5F5F5;color:#8B0000\">nltk</code>.</li>\n",
    "<li>Use the dependency parser, <code style=\"background-color: #F5F5F5;color:#8B0000\">dep_parse_sentences_arceager</code>, to parse your PoS tagged sentences to produce a list of parsed sentences.  \n",
    "Call this list <code style=\"background-color: #F5F5F5;color:#8B0000\">parsed_sentences</code>.   \n",
    "You will need to <code style=\"background-color: #F5F5F5;color:#8B0000\">import dep_parse_sentences_arceager</code> from <code style=\"background-color: #F5F5F5;color:#8B0000\">sussex_nltk.parse</code>.</li>\n",
    "<li>Iterate over parsed sentences printing each one.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now explain how to interpret a dependency parsed sentence.\n",
    "\n",
    "Once the tokens in a sentence have been parsed they will have five attributes:\n",
    "\n",
    "- ID: This is a unique ID assigned to the token (unique within each sentence).\n",
    "- FORM: This is the actual text of a token.\n",
    "- POS: This is the PoS tag assigned to the token.\n",
    "- HEAD: In this attribute, you will find the ID of the token on which the current token depends (the head of the current token).\n",
    "- DEPREL: This is the relation that holds between the current token and its head.  \n",
    "\n",
    "An example sentence is shown below. Notice the following properties of the sentence \"sat the cat sat on the mat\":\n",
    "\n",
    "- \"cat\" is a noun. Its head is \"sat\", and it's the subject of \"sat\" (nsubj relation).\n",
    "- \"sat\" has 3 dependents: \"cat\" (its subject), \"on\" (a preposition), and \".\" (punctuation).\n",
    "- There are two \"the\" tokens, one of which is a dependent of \"cat\" (with a determiner relation \"det\"), and the other is a dependent of \"mat\" (with a determiner relation \"det\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ID      FORM    POS     HEAD    DEPREL\n",
    "\n",
    "1       the     DT      2       det\n",
    "2       cat     NN      3       nsubj\n",
    "3       sat     VBD     0       root\n",
    "4       on      IN      3       prep\n",
    "5       the     DT      6       det\n",
    "6       mat     NN      4       pobj\n",
    "7       .       .       3       punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sentence represented in a dependency graph is shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FFDEAD;border:1px solid #DCDCDC;padding: 5px;\">\n",
    "NOTE   \n",
    "\n",
    "The arrows go FROM the head TO the dependent.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example dependency tree](./img/deptree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FAFAD2;color:#8B0000;border:1px solid #DCDCDC;padding: 5px;\"> \n",
    "<h3>Things for you to do</h3>\n",
    "<ul>\n",
    "<li>For each of your example sentences, draw the dependency tree that was produced by the parser. You should do this on a piece of paper.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency tree visualisation tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the teaching drive under <code style=\"background-color: #F5F5F5;\">Departments/Informatics/LanguageEngineering/</code>, there is a file called \"<code style=\"background-color: #F5F5F5;\">RunParserInLab4.bat</code>\". Double-click this file and it will run an interactive dependency parser.\n",
    "\n",
    "It performs two tasks, only one of which is relevant to you. In the pane labelled \"Plain\" you can copy-paste any <code style=\"background-color: #F5F5F5;\">ParsedSentence</code> print-out, then press SHIFT+ENTER. Tthen he dependency tree will be visualised.\n",
    "\n",
    "This may help you to understand the trees.\n",
    "\n",
    "If you would like to use the tool at home, you should instead use a copy of the <code style=\"background-color: #F5F5F5;\">InteractiveParser.jar</code> file from the same directory. Ensure that your home computer uses Java 7 (or later) at the terminal by default. Then at the command prompt type:\n",
    "\n",
    "<code style=\"background-color: #F5F5F5;\">java -Xmx2g -jar /path/to/InteractiveParser.jar</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FAFAD2;color:#8B0000;border:1px solid #DCDCDC;padding: 5px;\"> \n",
    "<h3>Things for you to do</h3>\n",
    "<ul>\n",
    "<li>Run the dependency tree visualisation tool on the same sentences that you experimented with in the previous section.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing the Amazon reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This sections shows how to get the parser to parse a selection of Amazon review sentences. The code can be used to do the following:  \n",
    "\n",
    "- Filter a category of the Amazon review corpus for only those sentences containing particular tokens\n",
    "- PoS tag the filtered sentences\n",
    "- Dependency parse and print the filtered sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sussex_nltk.parse import dep_parse_sentences_arceager       # Import the function which parses an iterable of sentences\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader  # Import the corpus reader\n",
    "from nltk import pos_tag                                         # Import the PoS tagging function\n",
    "\n",
    "# Create a list of reviews that contain the verb \"to buy\", \n",
    "# by filtering for several of its conjugations\n",
    "\n",
    "sentences = []\n",
    "verb_variants = set([\"buy\",\"buys\",\"bought\"])\n",
    "# You can use any product category (or even all product categories), \n",
    "# but DVD is small and takes less time to process\n",
    "for sentence in AmazonReviewCorpusReader().category(\"dvd\").sents(): \n",
    "# Check for several variations of the verb. \"&\" finds the intersection \n",
    "# of 2 sets (what they have in common). So the below statement says \n",
    "# \"if the set of sentence tokens has any token in common with our verbs, \n",
    "# then keep the sentence\"\n",
    "    if verb_variants & set(sentence):\n",
    "        sentences.append(sentence)  # Populate our list of sentences with \n",
    "                                    # this sentence if it contains what \n",
    "                                    # we're after.\n",
    "\n",
    "# Optionally limit the number of reviews to *n* for ease of processing \n",
    "# and observation. Or perform some kind of sampling (not shown)\n",
    "n = 10\n",
    "sentences = sentences[:n]  # Slice notation\n",
    "\n",
    "#Create an iterable over dependency parsed sentences\n",
    "# Round brackets create a generator instead of a list, see the link below \n",
    "# the code for an explanation of generator expressions. This means that the \n",
    "# sentences will only be PoS-tagged as and when the parser needs them, one \n",
    "# at a time.\n",
    "tagged_sents = (pos_tag(sentence) for sentence in sentences) \n",
    "parsed_sents = dep_parse_sentences_arceager(tagged_sents)\n",
    "\n",
    "# Now you can iterate over *parsed_sents* performing computations or printing\n",
    "for sentence in parsed_sents:\n",
    "    print \"------ Sentence ------\"\n",
    "    print sentence \n",
    "# This sentence is a ParsedSentence object, which is explained further in \n",
    "# the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FAFAD2;color:#8B0000;border:1px solid #DCDCDC;padding: 5px;\"> \n",
    "<h3>Things for you to do</h3>\n",
    "<ul>\n",
    "<li>Look through the code snippet above and make sure you understand how it works.</li>\n",
    "<li>Run the above code with different choices of review categories and different verb variants.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing the output of the parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function <code style=\"background-color: #F5F5F5;\">dep_parse_sentences_arceager</code> returns an list of sentences, where each sentence is in the form of a <code style=\"background-color: #F5F5F5;\">ParsedSentence</code> instance (which is a class defined in the <code style=\"background-color: #F5F5F5;\">sussex_nltk codebase</code>).\n",
    "\n",
    "Notice that you can iterate over a <code style=\"background-color: #F5F5F5;\">ParsedSentence</code> instance, getting its tokens. Each token is a <code style=\"background-color: #F5F5F5;\">BasicToken</code> instance (another class defined in the <code style=\"background-color: #F5F5F5;\">sussex_nltk</code> codebase), which keeps track of the attributes of a token. For example, if you have a <code style=\"background-color: #F5F5F5;\">BasicToken</code> instance called <code style=\"background-color: #F5F5F5;\">token</code>, you can call <code style=\"background-color: #F5F5F5;\">token.deprel</code> to get its dependency relation. The code snippet below illustrates the other properties a token has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This code assumes you have the *parsed_sents* and *verb_variants* variables \n",
    "# from the previous section. *parsed_sents* is a list of ParsedSentence objects\n",
    "\n",
    "# Print to screen the parsed sentences\n",
    "for sentence in parsed_sents: # *parsed_sents* acquired from the previous section\n",
    "    print \"-----\" # Just a separator\n",
    "    print sentence\n",
    "\n",
    "# Each sentence is made up of a list of BasicToken objects\n",
    "# Each token has several attributes: id, form (the actual word), pos, \n",
    "# head, deprel (dependency relation)\n",
    "    \n",
    "for token in parsed_sents[0]: # for each token in the first sentence\n",
    "    print token.id, token.form, token.pos, token.head, token.deprel\n",
    "\n",
    "# For each sentence, print all of the dependent tokens that are subjects "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The snippet below shows you how to iterate through the sentences printing only the dependents of certain tokens, and only if a certain relation holds (here \"nsubj\", but this could be any relation, e.g. \"dobj\").\n",
    "\n",
    "Note that those tokens which have \"dobj\" (direct object) relations with \"love\", \"buy\"\" and \"want\" will be the things that are loved, bought and wanted respectively. For example, in \"I love the cat, but will buy the fish\", \"cat\" is the direct object of \"love\", and \"fish\" is the direct object of \"buy\".\n",
    "\n",
    "The <code style=\"background-color: #F5F5F5;\">find_all_dependants</code> function is a method on the <code style=\"background-color: #F5F5F5;\">ParsedSentence</code> class, which takes as input an iterable over words. It will check the sentence for those words, and return all the dependents of all of the matches. The dependents will be <code style=\"background-color: #F5F5F5;\">BasicToken</code> instances. So if a sentence contains word A, then all tokens which list A as their head will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For each sentence, print all of the dependent tokens that are subjects \n",
    "# (nsubj relation) of your verb variants\n",
    "relation = 'nsubj'\n",
    "for sentence in parsed_sents:\n",
    "    print \"------ Sentence ------\"\n",
    "    print sentence \n",
    "# You could instead 'print sentence.raw()', which gives only the original \n",
    "# text and takes less room on the screen, but you won't be able to see \n",
    "# parsing errors.\n",
    "    \n",
    "    print 'Dependents with \"%s\" relation:' % relation, \\\n",
    "    [token.form for token in sentence.find_all_dependants(verb_variants) if token.deprel == relation]\n",
    "# print all of the dependents of the verbs that have the dependency \n",
    "# relation \"nsubj\", *verb_variants*. This uses the \"get_dependents\" \n",
    "# function of the ParsedSentence object. Given an iterable of words, \n",
    "# the function will return all of the dependents of those words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct object examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The direct object of a verb, is the recipient of the action. So in \"I bought Shrek\", \"Shrek\" is the direct object of a buying action. So when we're looking for the direct objects of \"want\", \"buy\" and \"love\" we're looking for the words which are wanted, bought and loved. The sentence says that \"Shrek\" was loved, but the parser doesn't mark \"Shrek\" as the direct object of \"love\", then it probably assigned the wrong relation.\n",
    "\n",
    "The following is a screenshot of a dependency tree in which the \"dobj\" relation is correctly attached. \"DVD\" is the word on the receiving end of the buying action, and it is marked as such. See that in the tree, an arrow goes from \"buy\" to \"DVD\" with the label \"dobj\". This is reflected in the text below the tree, where the token \"DVD\" says its head is token 20 (\"buy\") with the relation \"dobj\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Direct Object Example](./img/correctdobj.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of when \"dobj\" has been attached incorrectly. Notice the phrase \"I bought this\"; \"this\" is the thing being bought, so it should be marked as the \"dobj\" of \"bought\".\n",
    "\n",
    "Instead, it is marked as the subject of \"seems\" (the thing actually doing the \"seeming\"). This means the parser has interpreted it as \"...this seems more like...\", rather than \"...I bought this...\". It's much more likely that the author was saying that James Garner seems like a bystander (so Garner should be the subject of \"seems\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Incorrect Direct Object](./img/incorrectdobj.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FAFAD2;color:#8B0000;border:1px solid #DCDCDC;padding: 5px;\"> \n",
    "<h3>Things for you to do</h3>  \n",
    "\n",
    "Follow the 3 steps below to acquire information about certain relations. Could this information be useful in subsequent tasks (e.g. some form of information extraction)? What problems may arise when trying to use this information? Why? Is there anything that could be done to make it more useful?\n",
    "<ul>\n",
    "<li>Find reviews in the Amazon review corpus which contain the following verbs: \"love\", \"buy\", and \"want\".</li>\n",
    "<li>PoS tag them with the NLTK PoS tagger, and then dependency parse them.</li>\n",
    "<li>Write code to extract all of the tokens that appear as direct objects of those verbs (the relevant dependency relation is \"dobj\"). These should be the things that are being loved/bought/wanted.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section reminds you how to sample some random tweet sentences and tokenise and PoS tag them with the NLTK tools. It then shows you how dependency parse the sentences.\n",
    "\n",
    "This will allow you to investigate the performance of the parser on tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sussex_nltk.corpus_readers import TwitterCorpusReader\n",
    "from sussex_nltk.parse import dep_parse_sentences_arceager\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "tcr = TwitterCorpusReader()\n",
    "\n",
    "# Get some (here 30) un-tokenised sentences from tweets\n",
    "sents = tcr.sample_raw_sents(30) \n",
    "\n",
    "# Tokenise and PoS tag the sentences\n",
    "# Notice the round brackets instead of square brackets. This is a generator \n",
    "# expression. It acts quite like a list, but instead of computing all list \n",
    "# elements and storing all in memory, it only does one at a time.\n",
    "# Therefore \"tagged_sents\" is a generator, not a list\n",
    "tagged_sents = (pos_tag(word_tokenize(sentence)) for sentence in sents) \n",
    "\n",
    "# Dependency parse the sentences\n",
    "parsed_sents = dep_parse_sentences_arceager(tagged_sents)\n",
    "\n",
    "# Now you can inspect the results by printing the sentences as in the \n",
    "# previous section'''\n",
    "for sentence in parsed_sents:\n",
    "    print \"-----\"\n",
    "    print sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[A short explanation of generator expressions](https://wiki.python.org/moin/Generators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FAFAD2;color:#8B0000;border:1px solid #DCDCDC;padding: 5px;\"> \n",
    "<h3>Things for you to do</h3>\n",
    "<ul>\n",
    "<li>Assess the performance of the parser on a sample of tweets, and compare this with its performance on a sample of Amazon reviews. In particular, pick some common relations like <code style=\"background-color: #F5F5F5;color:#8B0000\">\"dobj\"</code> (direct object), <code style=\"background-color: #F5F5F5;color:#8B0000\">\"nsubj\"</code> (nominal subject), <code style=\"background-color: #F5F5F5;color:#8B0000\">\"amod\"</code> (adjectival modifier), and see how well they are assigned in the parsed tweets. Information on the different dependency relations can be found [here](http://nlp.stanford.edu/software/dependencies_manual.pdf).</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing tweets with Twitter-specific PoS tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section reminds you how to sample some random tweet sentences and tokenise and PoS tag them with the Twitter-specific tools. It then shows you how to dependency parse the sentences.\n",
    "\n",
    "This will allow you to investigate why the parser performs so poorly with the tags that should be more accurate on tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sussex_nltk.tag import twitter_tag_batch\n",
    "from sussex_nltk.corpus_readers import TwitterCorpusReader\n",
    "from sussex_nltk.parse import dep_parse_sentences_arceager\n",
    "\n",
    "tcr = TwitterCorpusReader()\n",
    "\n",
    "# Get some (here 30) un-tokenised sentences from tweets\n",
    "sents = tcr.sample_raw_sents(30) \n",
    "\n",
    "# PoS tag the sentences (remember the twitter tagger \n",
    "# also tokenises for you)\n",
    "tagged_sents = twitter_tag_batch(sents)\n",
    "\n",
    "# Dependency parse the sentences\n",
    "parsed_sents = dep_parse_sentences_arceager(tagged_sents)\n",
    "\n",
    "# Again, you have parsed sentences\n",
    "for sentence in parsed_sents:\n",
    "    print \"------\"\n",
    "    print sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FAFAD2;color:#8B0000;border:1px solid #DCDCDC;padding: 5px;\"> \n",
    "<h3>Things for you to do</h3>\n",
    "<ul>\n",
    "<li>PoS tag some tweets using the Twitter-Specific PoS-tagger. Why does the parser perform so poorly despite that fact that the PoS tagging is more accurate?</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [\"Incrementality in Deterministic Dependency Parsing\"](http://acl.ldc.upenn.edu/W/W04/W04-0308.pdf), a paper on transition-based parsing.\n",
    "- [Lecture series on data-driven dependency parsing in general](http://www.ryanmcd.com/courses/esslli2007/).\n",
    "- [Dependency Parsing book](http://www.morganclaypool.com/doi/abs/10.2200/s00169ed1v01y200901hlt002) (available for download by visiting link from Sussex machine) covers grammar- and data-driven dependency parsing, transition- and graph-based and many other related issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
