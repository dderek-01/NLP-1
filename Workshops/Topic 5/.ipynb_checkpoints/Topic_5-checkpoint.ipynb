{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 5: Dependency Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries \n",
    "Run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'\\\\ad.susx.ac.uk\\ITS\\TeachingResources\\Departments\\Informatics\\LanguageEngineering\\resources')\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from collections import defaultdict,Counter\n",
    "from itertools import zip_longest\n",
    "from IPython.display import display\n",
    "from random import seed\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pylab as pylab\n",
    "%matplotlib inline\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'large',\n",
    "         'axes.titlesize':'large',\n",
    "         'xtick.labelsize':'large',\n",
    "         'ytick.labelsize':'large'}\n",
    "pylab.rcParams.update(params)\n",
    "from pylab import rcParams\n",
    "from operator import itemgetter, attrgetter, methodcaller\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spaCy\n",
    "In the next topic, Topic 6, we will be investigating how to make a fine-grained analysis of the content of Amazon reviews. In particular, we will be analysing what the reviewer says about specific aspects of the product, e.g. what is said about the *plot* of a *film*. In preparation for that, in this notebook, we will be learning about dependency trees.\n",
    "\n",
    "Dependency trees allow us to see how the words in a sentence relate to one another grammatically. This contrasts with  what we've been doing up until now when determining the sentiment of a review; we've been viewing a document as an unordered bag of words.\n",
    "\n",
    "Up to this point we have been using varous NLP tools that form part of the NLTK. We now turn to an alternative, significantly more powerful NLP toolkit, one that provides state-of-the-art accuracy and state-of-the-art efficiency. \n",
    "\n",
    "We will be using something called [spaCy](https://spacy.io/). \n",
    "\n",
    "In this notebook we will be familiarising ourselves with many of the things that spaCy can do. \n",
    "\n",
    "Note that several of the examples that appear in this notebook have either been taken directly, or have been adapted from various spaCy tutorials found [here](https://spacy.io/docs/usage/tutorials).\n",
    "\n",
    "To load spaCy, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "# we'll be using the English version of spaCy. German, French, Spanish versions are also available.\n",
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review dataset\n",
    "We need some data to work with. Let's set up a dataset of dvd reviews, `dvd_reviews`. \n",
    "\n",
    "To do this, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n",
    "\n",
    "# create a list containing the raw text of all of the available dvd reviews.\n",
    "dvd_reviews = [review for review in AmazonReviewCorpusReader().category(\"dvd\").raw()]\n",
    "print(\"The dvd review dataset contains {} reviews\".format(len(dvd_reviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing a review with spaCy\n",
    "We now look at what spaCy produces when it analyses text. \n",
    "\n",
    "The following cell illustrates some (though by no means all) of the elements of the analysis. After being analysed by spaCy, much of the analysis is accessible through the tokens. Each token is an object that has a number of properties. See [here](https://spacy.io/docs/api/token) for a full list of a token's properties.\n",
    "\n",
    "Note: in general, when a property name ends with an underscore character, e.g.  `orth_`, that property returns a string (unicode) representation of the value for that property. This is useful when displaying output in a human-readable way. With no underscore, e.g. `orth`, the property returns the index of the value within the spaCy vocabulary.\n",
    "\n",
    "In the following cell, we see the following token properties being used:\n",
    "- `orth_`: the token's orthography.\n",
    "- `lemma_`: the uninflected form of the token.\n",
    "- `shape_`: the token shape.\n",
    "- `pos_`: the part of speech of the token.\n",
    "- `is_stop`: is the token a stop word or not?\n",
    "- `is_punc`: is the token punctuation or not?\n",
    "- `is_space`: is the token whitespace? Note that spacy tokeniser treates any sequence of whitespace characters beyond a single space as a token.\n",
    "- `like_num`: is the token a number?\n",
    "- `is_oov`: is the token an out-of-vocabulary word?\n",
    "- `prop`: the log probabilities of tokens, where the probabilities are estimated from a three billion word corpus, with simple Good-Turning smoothing;\n",
    "\n",
    "### Exercise\n",
    "Run the following cell several times so that you can look at the output for a variety of sentences.\n",
    "- Notice that parts of speech tags are upper case strings, e.g. `VERB`.\n",
    "- Look at places where the lemma is different from the token.\n",
    "- See if you can find sentences with out-of-vocabulary (oov) tokens.\n",
    "- What does the `shape` property capture?\n",
    "- See if you can figure out what this line is doing:\n",
    "```\n",
    "df.loc[:, 'stop?':'out of vocab.?'] = (df4.loc[:, 'stop?':'out of vocab.?']\n",
    "                                       .applymap(lambda x: 'Yes' if x else ''))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomly choose a review\n",
    "review = random.choice(dvd_reviews)\n",
    "#run spaCy on the review\n",
    "parsed_review = nlp(review) # in spaCy we call parsed_review a Doc\n",
    "\n",
    "# get just the first sentence of the review\n",
    "parsed_sentence = next(parsed_review.sents) # in spaCy we call parsed_sentence a Span (of a Doc)\n",
    "\n",
    "token_attributes = [(token.orth_,\n",
    "                     token.lemma_,\n",
    "                     token.pos_,\n",
    "                     token.like_num,\n",
    "                     token.is_stop,\n",
    "                     token.is_oov,\n",
    "                     token.is_punct,\n",
    "                     token.is_space,\n",
    "                     token.shape_,\n",
    "                     token.prob,\n",
    "                    )\n",
    "                    for token in parsed_sentence]\n",
    "\n",
    "df = pd.DataFrame(token_attributes,\n",
    "                   columns=['text',\n",
    "                            'lemma',\n",
    "                            \"pos\",\n",
    "                            'number?',\n",
    "                            'stop?',\n",
    "                            'oov?',\n",
    "                            'punctuation?',\n",
    "                            'whitespace?',\n",
    "                            'shape',\n",
    "                            'log probability',\n",
    "                           ])\n",
    "\n",
    "df.loc[:, 'number?':'whitespace?'] = (df.loc[:, 'number?':'whitespace?']\n",
    "                                       .applymap(lambda x: 'yes' if x else 'no'))\n",
    "\n",
    "print('Analysis of the sentence:\\n{}'.format(parsed_sentence.text))                                               \n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Properties of spaCy objects\n",
    "\n",
    "Three classes of objects make up a spaCy analysis:\n",
    "- A document.\n",
    "- A span. \n",
    " - this as a subsequence, or slice, of the parsed document and could be a sentence or phrase.\n",
    "- A token.\n",
    "\n",
    "Each of these has various properties.\n",
    "\n",
    "In each of the following three code cells you will see code that uses `dir` to display the full set of such properties for each kind of object. \n",
    "\n",
    "We begin with a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review = random.choice(dvd_reviews)\n",
    "parsed_review = nlp(review) # in spaCy we call parsed_review a Doc\n",
    "      \n",
    "for prop in dir(parsed_review):\n",
    "    if not prop.startswith('_'):\n",
    "        print(\"\\t\",prop)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we look at the properties of spans. In this case, our span is a sentence from the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review = random.choice(dvd_reviews)\n",
    "parsed_review = nlp(review) # in spaCy we call parsed_review a Doc\n",
    "parsed_sentence = next(parsed_review.sents) # in spaCy we call parsed_sentence a Span (of a Doc)\n",
    "\n",
    "for prop in dir(parsed_sentence):\n",
    "    if not prop.startswith('_'):\n",
    "        print(\"\\t\",prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Finally, we look at the properties of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "review = random.choice(dvd_reviews)\n",
    "parsed_review = nlp(review) # in spaCy we call parsed_review a Doc\n",
    "parsed_sentence = next(parsed_review.sents) # in spaCy we call parsed_sentence a Span (of a Doc)\n",
    "\n",
    "for prop in dir(parsed_sentence[0]):\n",
    "    if not prop.startswith('_'):\n",
    "        print(\"\\t\",prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependency trees in spaCy\n",
    "We are now ready to look at dependency trees.\n",
    "\n",
    "Dependency trees are graphs that are used to describe the syntax of a sentence. They do this by specifying relationship between the tokens in the sentence. The vertices of the graph are the tokens and the edges of the graph capture grammatical relationship between tokens, e.g. that a noun is the subject of a verb. They are called dependency **trees** because the graph is a tree.\n",
    "\n",
    "The following visualisation shows the a dependency tree produced by spaCy for the sentence  \n",
    "\"*However, the plot was predictable.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dependency tree example](./img/example_dependency_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exerise\n",
    "In order to get a sense of what dependency trees produced by spaCy look like, take a look at a demo of spaCy's parser \n",
    "which can be found [here](https://demos.explosion.ai/displacy).\n",
    "\n",
    "In the box at the top (the one with the magnifying glass icon on its right), type in a sentence, run the parser, and examine the output. Try this for a few sentences.\n",
    "\n",
    "Here are some things to look out for:\n",
    "- Across the bottom of the tree, you will see each token with its part-of-speech shown below.\n",
    " - A full list of the parts of speech tag set can be found [here](https://spacy.io/docs/api/annotation#pos-tagging).\n",
    " - The tokens are shown in the order that they appear in the text.\n",
    " - Use `spacy.explain` to get a brief explanatin of a symbol, e.g. try `spacy.explain(\"JJ\")`.\n",
    "- Above the tokens you see **directed, labelled edges**. \n",
    " - Each edge specifies a dependency between two tokens in the sentence.\n",
    " - It is the edges that provide the syntactic analysis of the sentence.  \n",
    " - Each edge connects a **head** with one of its **dependents**. \n",
    " - The edges are directed **from** the head **to** the dependent.\n",
    " - The edges are labelled by the name of the dependency relation. \n",
    " - A full set of dependency relations can be found [here](https://spacy.io/docs/api/annotation#dependency-parsing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with dependency trees in spaCy\n",
    "Tokens are associated with two properties that encode the dependency tree that spaCy has assigned to a sentence.\n",
    "- `token.head`: this gives the token in the sentence that is the head of this token.\n",
    "- `token.dep_`: this gives the label of the dependency relation that links `token.head` to `token`.\n",
    "\n",
    "Note that when `token` is the root of the dependency tree `token.head == token`.\n",
    "\n",
    "### Exerise\n",
    "Run the cell below and inspect the output. \n",
    "- Notice that dependency labels are lower case strings, e.g. `nsubj`.\n",
    "- Notice the token that is at the root of the dependency tree has itself as its head.\n",
    "- Type the same sentence into the [spaCy parser demo](https://demos.explosion.ai/displacy) and check that each line of output is compatible with the tree being displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# randomly choose a review\n",
    "review = random.choice(dvd_reviews)\n",
    "#run spaCy on the review\n",
    "parsed_review = nlp(review)\n",
    "# get just the first sentence of the review\n",
    "parsed_sentence = next(parsed_review.sents)\n",
    "\n",
    "token_attributes = [(token.orth_,\n",
    "                     token.pos_,\n",
    "                     token.dep_,\n",
    "                     token.head,\n",
    "                    )\n",
    "                    for token in parsed_sentence]\n",
    "\n",
    "df = pd.DataFrame(token_attributes,\n",
    "                   columns=['text',\n",
    "                            \"pos\",\n",
    "                            \"dep\",\n",
    "                            \"head\",\n",
    "                           ])\n",
    "                                               \n",
    "print('Analysis of the sentence:\\n{}'.format(parsed_sentence.text))\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the cell below, you will find code that shows the verb tokens in a review together with an indication of whether they appeared (at least once in the review) in an `nsubj` relation with another token.\n",
    "\n",
    "Make a copy of this cell, and in the new cell, adapt the code so that so that the output also includes an additional column showing whether the verb tokens also appeared in a sentence in a situation where the token had both a `nsubj` relationship with some other token and a `dobj` relation with yet some other token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = dvd_reviews[:10]\n",
    "for review in reviews:\n",
    "    parsed_review = nlp(review)\n",
    "    print(\"Review:\\n\\n{}\".format(review))\n",
    "    all_verbs = set()\n",
    "    verbs_with_nsubj = set()\n",
    "    for token in parsed_review:\n",
    "        if token.pos_ == 'VERB':\n",
    "            all_verbs.add(token)\n",
    "            for child in token.children:\n",
    "                if child.dep_ == 'nsubj':\n",
    "                    verbs_with_nsubj.add(token)\n",
    "                    break\n",
    "    print(\"Review:\\n{}\".format(review))\n",
    "    df = pd.DataFrame([(verb,verb in verbs_with_nsubj) for verb in all_verbs],\n",
    "                      columns=[\"verb\",'has nsubj?'])\n",
    "    df.loc[:, 'has nsubj?':'has nsubj?'] = (df.loc[:, 'has nsubj?':'has nsubj?']\n",
    "                                       .applymap(lambda x: 'yes' if x else ''))\n",
    "    display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "#%load solutions/verbs_with_subj_and_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Now adapt the code you wrote for the last exercise so that it displays a table with one column for each verb that appeared with at least one (`nsubj`,`dobj`) pair. The column for a verb should contains all the (`nsubj`,`dobj`) pairs that occurred with that verb.\n",
    "\n",
    "So a verb that occurred three times in the review in a situation where it had both an `nsubj` and a `dobj` would have entries in rows 0, 1 and 2, with each entry being the pair of tokens, i.e. the verbs `nsubj` and `dobj`. \n",
    "\n",
    "- Use a dictionary to store the (`nsubj`,`dobj`) pair details of each verb. \n",
    "- Store each verb's (`nsubj`,`dobj`) pairs in a list.\n",
    "- Put all of the lists of (`nsubj`,`dobj`) pairs into a list of lists of pairs, called `all_pairs`\n",
    "- Put the verbs in a list called `verbs` that is ordered in a way that aligns with the ordering in `all_pairs`.\n",
    "- Put this into a Pandas dataframe\n",
    " - use `pd.DataFrame(list(zip_longest(*all_pairs)),columns = verbs).applymap(lambda x: '' if x == None else x)` \n",
    " - see [unpack argument lists](https://docs.python.org/3/tutorial/controlflow.html#tut-unpacking-arguments) for an explanation of the `*`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "#%load solutions/verbs_with_subj_obj_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direct object relation\n",
    "The direct object of a verb, is the recipient of the action. So in \"I bought Shrek\", \"Shrek\" is the direct object of a buying action. So, for example, if we were to look for the direct objects of the verbs \"want\", \"buy\" and \"love\" we would find the words which are wanted, bought and loved. This relation is called `dobj`.\n",
    "\n",
    "### Exercise\n",
    "In the blank cell below, write code that finds all of the reviews in the DVD review set that contain the verbs \"love\", \"buy\" or \"want\". For each of these verbs, collect all the words that lie in the `dobj` relation with them, and show the results in a table. There should be one column for each of the three verbs.\n",
    "- To make the code general, do this: `target_verbs = [\"love\",\"buy\",\"want\"]`.\n",
    "- Our three target words are verb lemmas, so check their equality using `.lemma_`.\n",
    "- When you are debugging your code, don't run it on the whole dataset.\n",
    "- You can store the direct objects using a dictionary of lists and convert to a dataframe in the same way that was recommended for the previous exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "#%load solutions/love_buy_want_objs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
