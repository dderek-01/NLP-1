{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(r'/Users/davidw/Documents/teach/NLE/resources')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re    #import regex module\n",
    "\n",
    "def tokenise(sentence):\n",
    "    sentence = re.sub(\"'(s|m|(re)|(ve)|(ll)|(d))\\s\", \" '\\g<1> \",sentence + \" \")\n",
    "    sentence = re.sub(\"s'\\s\", \"s ' \",sentence)\n",
    "    sentence = re.sub(\"n't\\s\", \" n't \",sentence)\n",
    "    sentence = re.sub(\"gonna\", \"gon na\",sentence)\n",
    "    sentence = re.sub(\"\\\"(.+?)\\\"\", \"`` \\g<1> ''\",sentence)   \n",
    "    sentence = re.sub(\"([.,?!])\", \" \\g<1> \", sentence)\n",
    "    return sentence.split()\n",
    "\n",
    "def make_same_length(listA,listB):\n",
    "    # extends the shorter list (with empty strings) to make the lists have the same length\n",
    "    if len(listA) < len(listB):\n",
    "        listA.extend('' for _ in range(len(listB)-len(listA)))\n",
    "    else:\n",
    "        listB.extend('' for _ in range(len(listA)-len(listB)))\n",
    "    return listA,listB\n",
    "\n",
    "testsentence = \"After saying \\\"I won't help, I'm gonna leave it all to you!\\\", on his parents' arrival, the boy's behaviour suddenly improved.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 1: Preprocessing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*During the lab you should work your way through this document.*\n",
    "\n",
    "*Do each of the **Something for you to do** activities that appear along the way.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something for you to do\n",
    "\n",
    "*The first thing you need to do is run the following cell. This will give you access to the Sussex NLTK package.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Edit this cell to uncomment one line and remove the one that follows\n",
    "\n",
    "import sys\n",
    "#sys.path.append(r'T:\\Departments\\Informatics\\LanguageEngineering') \n",
    "sys.path.append(r'/Users/davidw/Documents/teach/NLE/resources')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A raw text document is just a sequence of characters. There are a number of basic steps that are often performed when processing natural language text. In this lab session we will cover some of the basic text pre-processing methods. In particular, you will be looking at:\n",
    "- <b> tokenisation</b> - roughly speaking, this involves grouping characters into words;\n",
    "- <b>case normalisation</b> - this involves converting all of the text into lower case; \n",
    "- <b>stemming</b> - this involves removing a word's inflections to find the stem; and \n",
    "- <b>punctuation and stop-word removal</b> - stop-words are common functions words that in some situations can be ignored.\n",
    "\n",
    "Note that we do not always apply all of the above preprocessing methods; it depends on the application. One of the things that you will be learning about in this module, is when the application of each of these methods is, and is not, appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided simple interfaces to each of the following corpora, which interact well with NLTK tools.\n",
    "\n",
    "- The NLTK texts\n",
    "- Amazon product reviews (~78k documents, ~640k sentences)\n",
    "- Wall Street Journal text (~2k documents, ~51k sentences)\n",
    "- Reuters articles (~61k documents, ~740k sentences)\n",
    "  - Reuters / Finance (~47k documents, ~550k sentences)\n",
    "  - Reuters / Sport (~13k documents, ~185k sentences)\n",
    "- Medline abstracts (~985k documents, ~6100k sentences)\n",
    "- Twitter posts (~962k documents, ~1720k sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting raw sentences from a corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corpora are too large to easily process with some of the functions you will be using, so we have provided a way for you to work on a randomly selected sample of each corpus.\n",
    "\n",
    "The Reuters, Twitter and Medline corpora have a function called <code style=\"background-color: #F5F5F5;\">sample_raw_sents</code>, which returns a specified number of random sentences, where each sentence is an un-tokenised string.\n",
    "\n",
    "The code in the next cell shows you how to iterate over a random sample of 10 sentences. When you are using a tokeniser, you will replace\n",
    "`# do something with sentence`\n",
    "with code that tokenises each sentence and prints the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sussex_nltk.corpus_readers import ReutersCorpusReader\n",
    "\n",
    "rcr = ReutersCorpusReader()    #Create a new reader\n",
    "\n",
    "sample_size = 10\n",
    "\n",
    "for sentence in rcr.sample_raw_sents(sample_size): #get a sample of random sentences, where each sentence is a string\n",
    "    # do something with sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something for you to do\n",
    "\n",
    "- Make a copy the cell above and move the copied cell so that it is positioned below this cell. \n",
    "- Adapt the code in the new cell so that it prints a sample of **20** sentences from the **Twitter** corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sussex_nltk.corpus_readers import TwitterCorpusReader\n",
    "\n",
    "tcr = TwitterCorpusReader()    #Create a new reader\n",
    "\n",
    "sample_size = 20\n",
    "\n",
    "for sentence in tcr.sample_raw_sents(sample_size): #get a sample of random sentences, where each sentence is a string\n",
    "    print(sentence)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something for you to do\n",
    "\n",
    "- Point your browser at [Sussex NLTK package documentation](http://www.sussex.ac.uk/Users/davidw/courses/nle/SussexNLTK-API/) and have a look around. This provides information about the above corpora. Take a particularly careful look at the [`corpus_readers` Module](http://www.sussex.ac.uk/Users/davidw/courses/nle/SussexNLTK-API/sussex_nltk.html#module-sussex_nltk.corpus_readers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something for you to do\n",
    "\n",
    "- In the code cell below write code that will establish whether there are systematic differences between the  average sentence length (as measured in terms of the number of characters in the sentence) of the sentences in the Reuters, Twitter and Medline corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your own solution in this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model solutions available (full and partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a partial solution\n",
    "#%load ../Solutions/3/average_sentence_length_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "# %load ../Solutions/3/average_sentence_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DIY Tokenisation with Regular Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text doesn't come in neat tokens ready for analysis, it must first undergo sentence segmentation and tokenisation.  \n",
    "We have already sentence segmented the corpora.  \n",
    "In this lab you will be focusing on tokenisation, in particular, you will be comparing the merits of the following tokenisers:  \n",
    "- Your own regular expression based tokeniser\n",
    "- The (NLTK implemented) PENN treebank style regular expression based tokeniser\n",
    "- A Twitter-specific CMU tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues to consider"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your goal when working through this next section should be to investigate the strengths and weaknesses of each of the 3 tokenisers on three rather different kinds of corpora: \n",
    "- the Reuters corpus, \n",
    "- the Twitter corpus and \n",
    "- the Medline corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making your own tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will write your own Python function, which takes as input a single string representing a sentence, and returns a <b>list of strings</b> obtained by splitting the sentence into tokens.\n",
    "\n",
    "Let's start by simply splitting by whitespace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'is', 'the', 'air-speed', 'velocity', 'of', 'an', 'unladen', 'swallow?']\n"
     ]
    }
   ],
   "source": [
    "print(\"   What    is the    air-speed   velocity of  an unladen swallow?   \".split()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something for you to do\n",
    "\n",
    "- In the empty code cell below write a [function](http://docs.python.org/tutorial/controlflow.html#defining-functions), `tokenise` which takes a sentence as input and returns a list of the tokens making up the sentence. Your first version of this function should tokenise only on whitespace, as shown in the cell above. Show that your function works on the sentence shown above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model solution available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "# %load ../Solutions/3/simple_tokenise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something for you to do\n",
    "\n",
    "- In the empty code cell below write code that applies your tokenise function to each sentence in a sample of 30 sentences taken from  the Reuters, Twitter and Medline corpora, 10 sentences from each corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model solution available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "# %load ../Solutions/3/tokenise_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most tokenisation policies (e.g. in the Wall Street Journal corpus), contractions like \"I'm\" tend to be split into \"I\" and \"'m\".  \n",
    "\n",
    "When it comes to more than just splitting by whitespace, it can be convenient to use [regular expressions](http://docs.python.org/library/re.html) to process the string in some way. The following code cell illustrates this. Trying running it and then read on to discover how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', \"'re\", 'using', 'coconuts', '!']\n"
     ]
    }
   ],
   "source": [
    "import re    #import regex module\n",
    "\n",
    "print(re.sub(\"([.?!'])\", \" \\g<1>\", \"You're using coconuts!\").split())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how the above code works by breaking it down.  \n",
    "\n",
    "First, run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You 're using coconuts!\n"
     ]
    }
   ],
   "source": [
    "import re    #import regex module\n",
    "\n",
    "print(re.sub(\"'\", \" '\", \"You're using coconuts!\")   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this code takes the string \"You're using coconuts!\" and inserts a space before the apostophe, the `'` character. \n",
    "\n",
    "Let's see how it works...\n",
    "\n",
    "The first argument of `re.sub`, i.e. `\"'\"`, is a regular expression that in this case is extremely simple, since it only matches the apostophe character, `'`.\n",
    "\n",
    "The second argument of `re.sub`, where we see `\" '\"`, indicates that an apostophe should be substituted by a space followed by an apostophe.\n",
    "\n",
    "Now let's make it slightly more complicated. We also want to insert a space before the `\"!\"`, so let's look at how to do that. \n",
    "\n",
    "Run the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You 're using coconuts !\n"
     ]
    }
   ],
   "source": [
    "import re    #import regex module\n",
    "\n",
    "print(re.sub(\"(['!])\", \" \\g<1>\", \"You're using coconuts!\")   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first argument of `re.sub`, has been changed to `\"(['!])\"`, which is a regular expression that matches either an apostophe character,`'`, or an exclamation mark,`!`.\n",
    "\n",
    "This is achieved with the regular expression `\"['!]\"`, where the square brackets enclose the alternative characters. \n",
    "\n",
    "Why does the regular expression contain parenthesis? \n",
    "\n",
    "It has to do with what we need to put as the second argument of `re.sub` where the substitution is specified. \n",
    "\n",
    "To understand this, you need to appreciate that we want to add a space before an apostrophe and also a space before an exclamation mark. How can we specify that in the second argument of `re.sub`? \n",
    "\n",
    "The answer is that we need to make use of the the idea of a **group**.\n",
    "\n",
    "The parenthesis in `\"(['!])\"` define the start and end of a group. In this case the whole regular expression is a group. In general, however, there can be several sets of parentheis defining several groups. For example, the regular expression `\"([Tt]h)e (m*n)\"` has two groups. Groups are numbered from left to right, so the group in the regular expression `\"(['!])\"` is group 1. \n",
    "\n",
    "Defining this group allows us to refer to the string that matches the regular expression `\"(['!])\"`, which will be either an apostrophe or an exclamation mark. This is then used in the second argument of `re.sub`, where we see `\" \\g<1>\"`, which indicates that the material that matches the apostophe or exclamation mark should be substituted by a space followed by the symbol that was matched. The `1` in `\\g<1>` tells us that it is group one.\n",
    "\n",
    "We are now ready to look at the original code, which is reproduced below and should now make sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', \"'re\", 'using', 'coconuts', '!']\n"
     ]
    }
   ],
   "source": [
    "import re    #import regex module\n",
    "\n",
    "print(re.sub(\"([.?!'])\", \" \\g<1>\", \"You're using coconuts!\").split())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the spaces are added before any full stop, question mark, exclamation mark or apostrophe.\n",
    "The resulting string is then split on white space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something for you to do\n",
    "\n",
    "- Create an empty code cell below, and write a new version of your `tokenise` function that uses `re.sub` in the way we've just considered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# put your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model solution available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What', 'is', 'the', 'air-speed', '.', 'velocity', 'of', 'an', 'unladen', 'swallow', '?']\n"
     ]
    }
   ],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "# %load ../Solutions/3/tokenise_with_re.sub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something for you to do\n",
    "\n",
    "\n",
    "- Create an empty code cell below, and extend your tokeniser function to cater for the following guidelines. \n",
    "- Test out your new tokeniser on the string  \n",
    "`\"After saying \\\"I won't help, I'm gonna leave!\\\", on his parents' arrival, the boy's behaviour improved.\"`  \n",
    " notice that the `\"` characters in the test sentence have been espaced, appearing as `\\\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guidelines\n",
    "\n",
    "- punctuation is split from adjoining words\n",
    "- opening double quotes are changed to two single forward quotes.\n",
    "- closing double quotes are changed to two single backward quotes.\n",
    "- the Anglo-Saxon genitive of nouns are split into their component morphemes, and each morpheme is tagged separately.\n",
    "  - e.g. `\"children's\"` produces `\"children 's\"`\n",
    "  - e.g. `\"parents'\"` produces `\"parents '\"`\n",
    "- contractions should be split into component morphenes\n",
    "  - e.g. `\"won't\"` produces `\"wo n't\"`\n",
    "  - e.g. `\"gonna\"` produces `\"gon na\"`\n",
    "  - e.g. `\"I'm\"` produces `\"I 'm\"`\n",
    "  \n",
    "  \n",
    "These tokenisation guidelines are a subset of those found [here](http://www.cis.upenn.edu/~treebank/tokenization.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Hints:\n",
    "\n",
    "- Use multiple calls to `re.sub` to deal with different cases one at a time. As in...\n",
    "\n",
    "```\n",
    "    sentence = re.sub(<pattern1>, <replacement1>,sentence)\n",
    "    sentence = re.sub(<pattern2>, <replacement2>,sentence)\n",
    "    sentence = re.sub(<pattern3>, <replacement3>,sentence)\n",
    "```\n",
    "\n",
    "- Order your calls to `re.sub` so that you deal with the specific cases first and the more general cases later.\n",
    "\n",
    "- In dealing with the replacement of start and end `\"`, you will find the following useful:\n",
    "\n",
    ">The `'*'`, `'+'`, and `'?'` qualifiers are all *greedy*; they match\n",
    ">as much text as possible.  Sometimes this behaviour isn't desired; if the RE\n",
    ">`<.\\*>` is matched against `<a> b <c>`, it will match the entire\n",
    ">string, and not just `<a>`.  Adding `'?'` after the qualifier makes it\n",
    ">perform the match in *non-greedy* or *minimal* fashion; as *few*\n",
    ">characters as possible will be matched.  Using the RE `<.\\*?>` will match\n",
    ">only `<a>`.  \n",
    "(taken from https://docs.python.org/2/library/re.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model solution available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['After', 'saying', '``', 'I', 'wo', \"n't\", 'help', ',', 'I', \"'m\", 'gon', 'na', 'leave', '!', \"''\", ',', 'on', 'his', 'parents', \"'\", 'arrival', ',', 'the', 'boy', \"'s\", 'behaviour', 'improved', '.']\n"
     ]
    }
   ],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "# %load ../Solutions/3/my_tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The NLTK regular expression tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK implements a regular expression tokeniser `word_tokenize` that is based on the above tokenisation guidelines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function**: `word_tokenize`\n",
    "\n",
    "- Arguments\n",
    " - a single string, representing a sentence\n",
    "- Returns\n",
    " - a list of strings, where each string is a token within the sentence</dd>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something for you to do\n",
    "\n",
    "- Make sure you understand the code in the cell below and then run it so that you can compare the way that the test sentence has been tokensed by the two tokenisers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%save -f ../Solutions/3/my_tokeniser 225"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         NLTK       MINE\n",
      "0       After      After\n",
      "1      saying     saying\n",
      "2          ``         ``\n",
      "3           I          I\n",
      "4          wo         wo\n",
      "5         n't        n't\n",
      "6        help       help\n",
      "7           ,          ,\n",
      "8           I          I\n",
      "9          'm         'm\n",
      "10        gon        gon\n",
      "11         na         na\n",
      "12      leave      leave\n",
      "13          !          !\n",
      "14         ``         ''\n",
      "15          ,          ,\n",
      "16         on         on\n",
      "17        his        his\n",
      "18    parents    parents\n",
      "19          '          '\n",
      "20    arrival    arrival\n",
      "21          ,          ,\n",
      "22        the        the\n",
      "23        boy        boy\n",
      "24         's         's\n",
      "25  behaviour  behaviour\n",
      "26   improved   improved\n",
      "27          .          . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Define a useful function for printing two sequences in a way that makes it easier to compare them\n",
    "def print_lists_in_columns(sequence1,sequence2,heading1,heading2):\n",
    "    \n",
    "    def make_lists_same_length(listA,listB):\n",
    "    # Extends the shorter list (with empty strings) to so that the lists have the same length\n",
    "        if len(listA) < len(listB):\n",
    "            listA.extend('' for _ in range(len(listB)-len(listA)))\n",
    "        else:\n",
    "            listB.extend('' for _ in range(len(listA)-len(listB)))\n",
    "        return listA,listB\n",
    "\n",
    "    sequence1,sequence2 = make_lists_same_length(sequence1,sequence2)\n",
    "    datadict = {heading1 : sequence1,\n",
    "                heading2: sequence2}\n",
    "    df = pd.DataFrame(datadict,columns=[heading1,heading2])\n",
    "    print(df,\"\\n\")\n",
    "\n",
    "testsentence = \"After saying \\\"I won't help, I'm gonna leave!\\\", on his parents' arrival, the boy's behaviour improved.\"\n",
    "\n",
    "# run the nltk tokeniser and your tokeniser on the test sentence\n",
    "nltk_toks = word_tokenize(testsentence) # run the nltk tokeniser\n",
    "my_toks = tokenise(testsentence) # run your tokeniser\n",
    "\n",
    "# print the sequences side by side\n",
    "print_lists_in_columns(nltk_toks,my_toks,'NLTK','MINE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something for you to do\n",
    "\n",
    "- In the code cell below write code to run both the `NLTK_Tokenise` and your own `Tokenise` function on a sample of 10 sentences from the Reuters corpus.\n",
    "- Use the `print_lists_in_columns` function (defined above) to show the two tokenisations of each sentence.\n",
    "- Look for differences in the output of the two tokenisers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load ../Solutions/3/nltk_vs_mine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Twitter-specific Tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third tokeniser for you to explore is a Twitter-specific tokeniser that has been developed by [Gimpel et al.](http://ttic.uchicago.edu/~kgimpel/papers/gimpel+etal.acl11.pdf) as part of a Twitter-specific part-of-speech tagger (featured in later lab classes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function**: `twitter_tokenize`\n",
    "- Arguments\n",
    " - a single string, representing a sentence\n",
    "- Returns\n",
    " - a list of strings, where each string is a token within the sentence</dd>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`twitter_tokenize` can be quite slow, so we have provided the following function to tokenise an entire sample of sentences at once.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function**: `twitter_tokenize_batch`\n",
    "- Arguments\n",
    " - a list of strings, where each string represents a sentence\n",
    "- Returns\n",
    " - a list of sentences, where each sentence is a list of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something for you to do\n",
    "- Create a new code cell and write code to run both  `twitter_tokenize` and the the NLTK tokeniser, `word_tokenize`, function on each sentence in a sample of 10 sentences from the Twitter corpus.\n",
    "- Display each sentence tokenised by the two tokenisers using the `print_lists_in_columns` function defined above.\n",
    "- Once you have done this, look for differences in the output of the two tokenisers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ../Solutions/3/nltk_vs_twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something for you to do\n",
    "- Re-using the code from the cell above, use both the NLTK and Twitter tokenisers on a sample of 10 sentences from the **Medline** corpus.\n",
    "- Look for situations where the  tokenisers do not tokenise appropriately.\n",
    "- Try to figure out the differences in tokenisation policies of the tokenisers.\n",
    "- Think about possible motivations for the differences in tokenisation policy, by considering how the tokens may be used in subsequent (down-stream) language processing steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ../Solutions/3/nltk_vs_twitter_medline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalising text and removing unimportant tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number and case normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without any kind of normalisation, the tokens `\"help\"` and `\"Help\"` are two distinct types. This may, or may not be what you want. \n",
    "\n",
    "Another example, is that `\"1998\"` and `\"1999\"` count as distinct types. There are situations where there is no need to distinction between different numbers.\n",
    "\n",
    "Python provides a [number of functions](http://docs.python.org/library/stdtypes.html#string-methods), which you can call in order to analyse their content, or produce new strings from them.\n",
    "\n",
    "The following code performs case normalisation and replaces tokens that consist of digits by \"NUM\". \n",
    "\n",
    "It uses [list comprehension](http://docs.python.org/tutorial/datastructures.html#list-comprehensions) to build a new list by looping through and filtering items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'cake', 'is', 'a', 'lie']\n",
      "['in', 'the', 'year', 'NUM', 'of', 'the', 'fourth', 'age', ',', 'after', 'NUM', 'years', 'as', 'king', ',', 'aragorn', 'died', 'at', 'the', 'age', 'of', 'NUM']\n"
     ]
    }
   ],
   "source": [
    "tokens = [\"The\",\"cake\",\"is\",\"a\",\"LIE\"]      #a list of tokens, some of which contain uppercase letters\n",
    "print([token.lower() for token in tokens])   #print newly created list of all lowercase tokens\n",
    "\n",
    "numbers = ['in', 'the', 'year', '120', 'of', 'the', 'fourth', 'age', ',', 'after', '120', 'years', 'as', 'king', ',' , 'aragorn', 'died', 'at', 'the', 'age', 'of', '210']\n",
    "print([\"NUM\" if token.isdigit() else token for token in numbers])  #replace all number tokens with \"NUM\" in a new list of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something for you to do\n",
    "- Create a new code cell below into which you should put code that normalises tokens such as `\"4th\"`, `\"1st\"` and `\"22nd\"` to `\"Nth\"`.\n",
    "- Try to adapt this code from the cell above: `[\"NUM\" if token.isdigit() else token for token in numbers]`\n",
    "- Test your code on the list `[\"The\", \"1st\", \"and\", \"2nd\", \"placed\", \"runners\", \"lapped\", \"the\", \"5th\",\".\"]`. \n",
    "- Check that the token `\"and\"` isn't changed to `\"Nth\"`.\n",
    "- You will find [this page](http://docs.python.org/library/stdtypes.html#string-methods) useful.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ../Solutions/3/normalise_to_Nth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something for you to do\n",
    "- Complete the code in the cell below. You have just two lines to complete. The goal is to use a large sample of the Reuters corpus to establish the extent to which vocabulary size is reduced when number and case normalisation is applied.\n",
    "- For each of the two incomplete lines you should use nested list comprehensions. This is described in Section 5.1.4 in [this document](http://docs.python.org/tutorial/datastructures.html#list-comprehensions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sussex_nltk.corpus_readers import ReutersCorpusReader\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def vocabulary_size(sentences):\n",
    "    tok_counts = collections.defaultdict(int)\n",
    "    for sentence in sentences: \n",
    "        for token in sentence:\n",
    "            tok_counts[token] += 1\n",
    "    return len(tok_counts.keys())\n",
    "\n",
    "rcr = ReutersCorpusReader()    \n",
    "\n",
    "sample_size = 10000\n",
    "\n",
    "raw_sentences = rcr.sample_raw_sents(sample_size)\n",
    "tokenised_sentences = [word_tokenize(sentence) for sentence in raw_sentences]\n",
    "\n",
    "############################################\n",
    "lowered_sentences = # complete this line\n",
    "normalised_sentences = # complete this line\n",
    "############################################\n",
    "\n",
    "raw_vocab_size = vocabulary_size(tokenised_sentences)\n",
    "normalised_vocab_size = vocabulary_size(normalised_sentences)\n",
    "print(\"Normalisation produced a {0:.2f}% reduction in vocabulary size from {1} to {2}\".format(\n",
    "    100*(raw_vocab_size - normalised_vocab_size)/raw_vocab_size,raw_vocab_size,normalised_vocab_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ../Solutions/3/impact_of_normalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A considerable amount of the apparent lexical variation found in documents results from the use of morphological variants which do not have a major impact on the topic of the document. An easy way to remove these varied forms is to use a stemmer. NLTK includes a number of stemmers in the <code style=\"background-color: #F5F5F5;\">nltk.stem</code> package.\n",
    "\n",
    "[NLTK stem module API](http://nltk.org/api/nltk.stem.html)\n",
    "\n",
    "[NLTK Porter stemmer](http://nltk.org/api/nltk.stem.html?highlight=stemmer#nltk.stem.porter.PorterStemmer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something for you to do\n",
    "- Complete the code below to show how the NLTK implementation of the Porter stemmer in `nltk.stem.porter.PorterStemmer` stems a sample of sentences in the Reuters corpus. All you need to do is to provide the missing first two arguments to the call to `print_lists_in_columns`.\n",
    "- Have a close look at the differences between the columns. This will give you a good indication of what the stemmer does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sussex_nltk.corpus_readers import ReutersCorpusReader\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "rcr = ReutersCorpusReader() \n",
    "st = PorterStemmer()\n",
    "\n",
    "sample_size = 10\n",
    "\n",
    "raw_sentences = rcr.sample_raw_sents(sample_size)\n",
    "tokenised_sentences = [word_tokenize(sentence) for sentence in raw_sentences]\n",
    "\n",
    "for sentence in tokenised_sentences:\n",
    "    print_lists_in_columns(<Argument 1>,<Argument 2>,\"BEFORE\",\"AFTER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ../Solutions/3/show_stemmer_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something for you to do\n",
    "- By looking at the impact on a large sample of the Reuters corpus, establish the extent to which vocabulary size is reduced by stemming.\n",
    "- Create a new code cell for this. You should be abnle to re-use a lot of the code from the code you used when measuring the impact of lower case and number normalisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ../Solutions/3/impact_of_stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation and stop-word removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stopword is a word that occurs so often that it loses its usefulness in some tasks. We may get more meaningful information from our corpus analysis if we remove stopwords and punctuation.\n",
    "\n",
    "The code below takes a list of tokens and creates a new list, which contains only those strings which are alphabetic and non-stop-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "filtered_tokens = [w for w in tokens if w.isalpha() and w not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**\n",
    "\n",
    "`isalpha` only returns `True` if the string is entirely composed of alphabet characters. If you want a function to return `True` even when a word contains digits, then you should use `isalnum`.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something for you to do\n",
    "- By looking at the impact on a large sample of the Medline corpus, establish what proportion of tokens are stop-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load ../Solutions/3/impact_of_stopword_removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Organising your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #FAFAD2;color:#8B0000;border:1px solid #DCDCDC;padding: 5px;\">\n",
    "<h3>Something for you to do</h3>\n",
    "<ul>\n",
    "<li><span style=\"color:#8B0000\">Use the test sentence in the cell below to test that you have satisfied the guidelines.</span></li>\n",
    "\n",
    "Notice that the double quotes within the string have been escaped using `\\`.\n",
    "\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you progress through these notebooks, you are creating (and we are providing) code snippets that perform various task that you may want to re-use at a later point. Instead of copy pasting code from lab sheets, it is better to build a library of functions that is easy for you to access.\n",
    "\n",
    "To do this, create another python module (see below how this is done) that will contain frequently used functions (you can call it <code style=\"background-color: #F5F5F5;\">nle_utils</code>). For instance the stop-word removal code later on in this lab session is something you will probably need later on in the module. Instead of copy pasting the code everytime you need it, you can separate the code into a function in <code style=\"background-color: #F5F5F5;\">nle_utils</code> and use it by importing the module.\n",
    "\n",
    "<code style=\"background-color: #F5F5F5;\">import nle_utils</code>  \n",
    "<code style=\"background-color: #F5F5F5;\">nle_utils.remove_stopwords('some text with stopwords')</code>\n",
    "\n",
    "For more information see:  \n",
    "- [Python functions](http://docs.python.org/2/tutorial/controlflow.html#defining-functions)\n",
    "- [Python modules](http://docs.python.org/2/tutorial/modules.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving your code with iPython magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save the code in one or more of the cells in a notebook using the iPython magic command <code style=\"background-color: #F5F5F5;\">save</code>. For example:\n",
    "\n",
    "- <code style=\"background-color: #F5F5F5;\">%save nle_utils.py 14</code> will save the contents of cell 14 to file <code style=\"background-color: #F5F5F5;\">nle_utils.py</code>.\n",
    "- <code style=\"background-color: #F5F5F5;\">%save nle_utils.py 1-10 12 15</code> will save contents of cells 1-10, 12 and 15 to file <code style=\"background-color: #F5F5F5;\">nle_utils.py</code>.\n",
    "\n",
    "Note that a cell is assigned a number when it is executed.\n",
    "\n",
    "Note that this will overwrite the file.\n",
    "\n",
    "To execute an iPython magic command, place it in a cell and run that cell.\n",
    "\n",
    "if you want to edit this code load the code back into an iPython notebook cell. To do this, place the following in a code cell and run the cell. The code will then appear in a new code cell.\n",
    "\n",
    "<code style=\"background-color: #F5F5F5;\">%load nle_utils.py</code>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
