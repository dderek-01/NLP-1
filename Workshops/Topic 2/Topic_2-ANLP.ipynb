{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 2: Basic Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries \n",
    "Run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name zip_longest",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-20ba427434cd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzip_longest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name zip_longest"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r'/Users/warrenboult/Documents/MSC/nlp/resources/')\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from collections import defaultdict,Counter\n",
    "from itertools import zip_longest\n",
    "from IPython.display import display\n",
    "from random import seed\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pylab as pylab\n",
    "%matplotlib inline\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'large',\n",
    "         'axes.titlesize':'large',\n",
    "         'xtick.labelsize':'large',\n",
    "         'ytick.labelsize':'large'}\n",
    "pylab.rcParams.update(params)\n",
    "from pylab import rcParams\n",
    "from operator import itemgetter, attrgetter, methodcaller\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are working on your own computer, update the following to contain the correct path for sussex_nltk and then run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('/Users/warrenboult/Documents/MSC/nlp/resources/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview \n",
    "This topic (Topic 2) and Topic 3 concern the task of sentiment analysis. You will be using a corpus of **book reviews** within an **Amazon review corpus**.\n",
    "\n",
    "You be exploring various techniques that can be used to classify the sentiment of Amazon book reviews as either positive or negative. \n",
    "\n",
    "You will be developing **Word List** classifiers and then comparing them to **Naïve Bayes** classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating training and testing sets\n",
    "During the next two lab sessions you will be training and testing various document classifiers. It is essential that the data used in the testing phase is not used during the training phase, since this can lead to overestimating performance. \n",
    "\n",
    "We now introduce the `split_data` function (defined in the cell below) which can be used to get separate **training** and **testing** sets.\n",
    "\n",
    "### Exercise\n",
    "Look through the code in the following cell, reading the comments and making sure that you understand each line.  You might want to run it with different ratios and measure the size of the different splits returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sussex NLTK root directory is', '/Users/warrenboult/Documents/MSC/nlp/resources')\n",
      "1400\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "from random import sample # have a look at https://docs.python.org/3/library/random.html to see what random.sample does\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n",
    "\n",
    " \n",
    "def split_data(data, ratio=0.7): # when the second argument is not given, it defaults to 0.7\n",
    "    \"\"\"\n",
    "    Given corpus generator and ratio:\n",
    "     - partitions the corpus into training data and test data, where the proportion in train is ratio,\n",
    "\n",
    "    :param data: A corpus generator.\n",
    "    :param ratio: The proportion of training documents (default 0.7)\n",
    "    :return: a pair (tuple) of lists where the first element of the \n",
    "            pair is a list of the training data and the second is a list of the test data.\n",
    "    \"\"\"\n",
    "    \n",
    "    data = list(data) # data is a generator, so this puts all the generated items in a list\n",
    " \n",
    "    n = len(data)  #Found out number of samples present\n",
    "    train_indices = sample(range(n), int(n * ratio))          #Randomly select training indices\n",
    "    test_indices = list(set(range(n)) - set(train_indices))   #Randomly select testing indices\n",
    " \n",
    "    train = [data[i] for i in train_indices]           #Use training indices to select data\n",
    "    test = [data[i] for i in test_indices]             #Use testing indices to select data\n",
    " \n",
    "    return (train, test)                       #Return split data\n",
    " \n",
    "#Create an Amazon corpus reader pointing at only book reviews\n",
    "book_reader = AmazonReviewCorpusReader().category(\"dvd\")\n",
    "\n",
    "#The following two lines use the documents function on the Amazon corpus reader. \n",
    "#This returns a generator over reviews in the corpus. \n",
    "#Each review is an instance of a Python class called AmazonReview. \n",
    "#An AmazonReview object contains all the data about a review.\n",
    "pos_train, pos_test = split_data(book_reader.positive().documents())\n",
    "neg_train, neg_test = split_data(book_reader.negative().documents())\n",
    "\n",
    "#You can also combine the training data\n",
    "train = pos_train + neg_train\n",
    "test = pos_test + neg_test\n",
    "print len(train)\n",
    "print len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating word lists\n",
    "The next section will explain how to use a sentiment classifier that bases its decisions on word lists. The classifier requires a list of words indicating positive sentiment, and a second list of words indicating negative sentiment. Given positive and negative word lists, a document's overall sentiment is determined based on counts of occurrences of words that occur in the two lists. In this section we are concerned with the creation of the word lists. We will be considering both hand-crafted lists and automatically generated lists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Create a reasonably long hand-crafted list of words that you think indicate positive sentiment.\n",
    "- Create a reasonably long hand-crafted list of words that indicate negative sentiment.\n",
    "\n",
    "Use the following cells to store these lists in the variables `my_positive_word_list` and `my_negative_word_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_positive_word_list = [\"good\",\"great\",\"lovely\",\"brilliant\",\"excellent\",\"wonderful\",\"funny\",\"amazing\",\"incredible\",\"exciting\"] # extend this with your own examples\n",
    "my_negative_word_list = [\"bad\", \"terrible\", \"awful\",\"worst\",\"horrendous\",\"dull\",\"boring\",\"lame\"] # extend this with your own examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets evaluate these intuitions by seeing how frequently they occur in the positive and negative training examples.  We first get a list of all the words in each set of training examples (using `get_all_words(review_set)`) and then construct the frequency distribution using the [NLTK <code style=\"background-color: #F5F5F5;\">FreqDist</code>](http://www.nltk.org/api/nltk.html#module-nltk.probability) object. \n",
    "\n",
    "### Exercise\n",
    "Make sure that you understand the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist # see http://www.nltk.org/api/nltk.html#module-nltk.probability\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n",
    "from functools import reduce # see https://docs.python.org/3/library/functools.html\n",
    "\n",
    "#Helper function. Given a list of reviews, return a list of all the words in those reviews\n",
    "#To understand this look at the description of functools.reduce in https://docs.python.org/3/library/functools.html\n",
    "def get_all_words(amazon_reviews):\n",
    "    return reduce(lambda words,review: words + review.words(), amazon_reviews, [])\n",
    "\n",
    "#A frequency distribution over all words in positive book reviews\n",
    "pos_freqdist = FreqDist(get_all_words(pos_train))\n",
    "neg_freqdist = FreqDist(get_all_words(neg_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the blank code cell below write code that uses the frequency lists, `pos_freqdist` and `neg_freqdist`, created in the above cell and `my_positive_word_list` and `my_negative_word_list` that you manually created earlier to determine whether or not the review data conforms to your expectations. In particular, whether:\n",
    "- the words you expected to indicate positive sentiment actually occur more frequently in positive reviews than negative reviews\n",
    "- the words you expected to indicate negative sentiment actually occur more frequently in negative reviews than positive reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'writings', u'Bogie', u'Heights', u'four', u'prices', u'woods', u'Olympics', u'yet.Its', u'Sellek', u'hanging', u'corrider', u'beatnik-style', u'bio-pics', u'Until', u'treading', u'granting', u'electricity', u'DISAPPOINTED', u'JOINED', u'wizardry', u'originality', u'Vulcan', u'Western', u'crooner', u'sputter', u'lord', u'meadows', u'sinking', u'Frankly', u'discribed', u'oceans', u'tinkerings', u'copout', u'replaces', u'tantalizing', u'leisurely', u'stabbed', u'bringing', u'elevates', u'basics', u'liaisons', u'grueling', u'Less', u'Bandit', u'Classic', u'Bacon', u'protest', u'Rojas', u'Does', u'Desperate', u'Tuesday', u'Paul', u'Holomade', u'commented', u'elegy', u'Current', u'semi-mainstream', u'tired', u'tolerate', u'preface', u'scrapes', u'Matthew', u'second', u'1938-1981', u'loathing', u'Ca', u'admire', u'Berlin', u'forgetting', u'Lucille', u'cooking', u'fingers', u'Companion', u'fossil', u'designing', u'Hooper', u'increasing', u'groupie', u'succumb', u'Presidential', u'pioneering', u'specialist', u'hero', u'Histoy', u'reporter', u'intentioned', u'gayer', u'fervent', u'Church', u'gun-toting', u'here', u'herd', u'ashen-faced', u'BETTY', u'Orcom', u'cult', u'parenting', u'previews', u'confronts', u'natured']\n"
     ]
    }
   ],
   "source": [
    "print list(pos_freqdist)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/check_expectations_partial\n",
    "def check_expectations(word_list,freqdist1,freqdist2,headers):\n",
    "    match_freq = [freqdist1[word] for word in word_list]\n",
    "    mismatch_freq = [freqdist2[word] for word in word_list]\n",
    "    as_expected = [match_freq[i]>mismatch_freq[i] for i in range(len(word_list))]\n",
    "    headers.append('Expected?')\n",
    "    df = pd.DataFrame(list(zip_longest(word_list,match_freq,mismatch_freq,as_expected)), columns=headers)\n",
    "    display(df,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load solutions/check_expectations\n",
    "def check_expectations(word_list,freqdist1,freqdist2,headers):\n",
    "    match_freq = [freqdist1[word] for word in word_list]\n",
    "    mismatch_freq = [freqdist2[word] for word in word_list]\n",
    "    as_expected = [match_freq[i]>mismatch_freq[i] for i in range(len(word_list))]\n",
    "    headers.append('Expected?')\n",
    "    df = pd.DataFrame(list(zip_longest(word_list,match_freq,mismatch_freq,as_expected)), columns=headers)\n",
    "    display(df,\"\\n\")\n",
    "\n",
    "headers = [\"Pos Word\",\"Freq in Pos\", \"Freq in Neg\"]\n",
    "check_expectations(my_positive_word_list,pos_freqdist,neg_freqdist,headers)\n",
    "headers = [\"Neg Word\",\"Freq in Neg\", \"Freq in Pos\"]\n",
    "check_expectations(my_negative_word_list,neg_freqdist,pos_freqdist,headers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deriving Word Lists\n",
    "\n",
    "Now we want to try to derive word lists automatically from the training examples.  One way to do this is to use the most frequent words in positive reviews as your positive list, and the most frequent words in negative reviews as your negative list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "The cell below is a copy of the cell that appeared just before the last exercise.\n",
    "\n",
    "Add two functions to this code.\n",
    "\n",
    "- `most_frequent_words` - this function should take two arguments: a frequency distribution and a natural number, k. It should return the top k most frequent  words in the frequency distribution. You can use the `most_common` method for this, but you will need to aware of what this method returns.\n",
    "- `words_above_threshold` - this function also takes two arguments: a frequency distribution and a natural number, k. It should return all of the words that have a frequency greater than k.\n",
    "\n",
    "Remove punctuation and stopwords from consideration. You can re-use code from near the end of Topic 1 notebook.\n",
    "Using the training data, create two sets of positive and negative word lists using these functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     I    The  movie   film    one     It   This   like    DVD  great   good   well   time  would    see    get  first really   also   much  story  watch   love   best   even   many    But     If people     In    two    way    And      A     He  There  still  never  think   make movies   know  music   show    THE   life   made  films  years  could \n",
      "  1852    937    676    634    510    370    361    321    288    281    272    224    223    218    209    202    201    192    184    179    179    176    170    164    163    160    155    149    148    143    138    137    137    134    134    132    127    125    123    121    118    118    117    115    114    111    111    110    110    110 \n",
      "None\n",
      "\n",
      "\n",
      "\n",
      "         I      movie        The       film        one       like         It      would       This        DVD       good        get     really       even       time      could       much        bad     people       make        see     movies      think     better      first      watch         If      story        way       know        two      great      There  character        But characters     acting       seen       made       plot        say       show      never  something       want        And     scenes        end    nothing       book \n",
      "      2113        862        810        548        435        410        330        307        307        296        266        230        221        217        214        200        196        185        170        169        167        163        157        155        151        145        144        143        138        134        129        128        128        128        126        124        117        116        116        115        112        112        109        109        107        105        103        103        102        100 \n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## from nltk.probability import FreqDist # see http://www.nltk.org/api/nltk.html#module-nltk.probability\n",
    "from sussex_nltk.corpus_readers import AmazonReviewCorpusReader\n",
    "from functools import reduce # see https://docs.python.org/3/library/functools.html\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Helper function. Given a list of reviews, return a list of all the words in those reviews\n",
    "#To understand this look at the description of functools.reduce in https://docs.python.org/3/library/functools.html\n",
    "def get_all_words(amazon_reviews):\n",
    "    return reduce(lambda words,review: words + review.words(), amazon_reviews, [])\n",
    "\n",
    "#A frequency distribution over all words in positive book reviews\n",
    "pos_freqdist = FreqDist(get_all_words(pos_train))\n",
    "neg_freqdist = FreqDist(get_all_words(neg_train))\n",
    "# print pos_freqdist.tabulate(20)\n",
    "# print neg_freqdist.tabulate(20)\n",
    "\n",
    "# print get_all_words(pos_train)\n",
    "# Remove punctuation and stopwords\n",
    "pos_train_filtered = [w for w in get_all_words(pos_train) if w.isalpha() and w not in stopwords.words(\"english\")]\n",
    "pos_freqdist = FreqDist(pos_train_filtered)\n",
    "print pos_freqdist.tabulate(50)\n",
    "print '\\n\\n'\n",
    "neg_train_filtered = [w for w in get_all_words(neg_train) if w.isalpha() and w not in stopwords.words(\"english\")]\n",
    "neg_freqdist = FreqDist(neg_train_filtered)\n",
    "print neg_freqdist.tabulate(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "# %load solutions/make_word_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a word list based classifier\n",
    "Now you have a number of word lists for use with a classifier. The following code can be used as the basis for creating a word list based classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.classify.api import ClassifierI\n",
    "import random\n",
    "\n",
    "class SimpleClassifier(ClassifierI): \n",
    "\n",
    "    def __init__(self, pos, neg): \n",
    "        self._pos = pos \n",
    "        self._neg = neg \n",
    "\n",
    "    def classify(self, words): \n",
    "        score = 0\n",
    "        \n",
    "        # add code here that assigns an appropriate value to score\n",
    "        \n",
    "        return \"N\" if score < 0 else \"P\" \n",
    "\n",
    "    def batch_classify(self, docs): \n",
    "        return [self.classify(doc.words() if hasattr(doc, 'words') else doc) for doc in docs] \n",
    "\n",
    "    def labels(self): \n",
    "        return (\"P\", \"N\")\n",
    "\n",
    "#Example usage:\n",
    "\n",
    "classifier = SimpleClassifier(top_pos, top_neg)\n",
    "classifier.classify(\"I read the book\".split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Copy the above code cell and move it to below this one. Then complete the `classify` method in the above code as specified below.\n",
    "- Test your classifier on several very simple hand-crafted examples to verify that you have implemented `classify` correctly.\n",
    "\n",
    "The classifier is initialised with a list of positive words, and a list of negative words. The words of a document are passed to the `classify` method (which is partially completed in the above code fragment). The `classify` method should be defined so that each occurrence of a negative word decrements `score`, and each occurrence of a positive word increments `score`. \n",
    "- For `score` less than 0, an \"`N`\" for negative should be returned.\n",
    "- For `score` greater than 0,  \"`P`\" for positive should returned.\n",
    "- For `score` of 0, the classification decision should be made randomly (see https://docs.python.org/3/library/random.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/simple_classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating word list based classifier\n",
    "Below is code that uses an evaluation function in order to determine how well your classifier performs. The function returns the <b>accuracy</b> of a classifier. The accuracy metric is defined as the proportion of documents that were correctly classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sussex_nltk.stats import evaluate_wordlist_classifier\n",
    "\n",
    "#Create a new classifier with your words lists\n",
    "book_classifier = SimpleClassifier(top_pos, top_neg)\n",
    "\n",
    "#Evaluate classifier\n",
    "#The function requires three arguments:\n",
    "# 1. Word list based classifer\n",
    "# 2. A list (or generator) of positive AmazonReview objects\n",
    "# 3. A list (or generator) of negative AmazonReview objects\n",
    "score = evaluate_wordlist_classifier(book_classifier, pos_test, neg_test)  \n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise  \n",
    "\n",
    "Use the blank cell below to perform the following two experiments:\n",
    "- Evaluate the performance of a classifier using hand-crafted lists.\n",
    "- Evaluate the performance of a classifier using lists derived from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/classifier_test\n",
    "from sussex_nltk.stats import evaluate_wordlist_classifier\n",
    "\n",
    "experiments = [(\"Hand-Crafted lists\",my_positive_word_list,my_negative_word_list),\n",
    "               (\"Top-k lists\",top_pos,top_neg),\n",
    "               (\"Above-k lists\",above_pos,above_neg)]\n",
    "\n",
    "\n",
    "for description,pos_list,neg_list in experiments:\n",
    "    #Create a new classifier with your words lists\n",
    "    classifier = SimpleClassifier(pos_list, neg_list)\n",
    "    score = evaluate_wordlist_classifier(classifier, pos_test, neg_test)\n",
    "    print(\"The accuracy of the {0} classifer is {1:.2f}\".format(description,score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes classifiers\n",
    "We now turn to a different model of document classification known as Naïve Bayes.\n",
    "\n",
    "Before we apply them to Amazon reviews, we need to implement them!\n",
    "\n",
    "We will introduce them through a very simple example dataset involving documents that are either about the weather or football. The classifier will be trained to distinguish these two topics from one another.\n",
    "\n",
    "There are, therefore, two classes `weather` and `football`. The classifier's job is to determine whether a document that it is given is in the class `weather` or in the class `football`.\n",
    "\n",
    "We give the classifier examples of documents in the `weather` class, and examples of documents in the `football` class. For now, to keep things simple, our so-called documents will be very short phrases.\n",
    "\n",
    "Run the following cell to set up some training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some test data to get us started. It is deliberately VERY simple. \n",
    "\n",
    "weather_sents_train = [\n",
    "    \"today it is raining\",\n",
    "    \"looking cloudy today\",\n",
    "    \"it is nice weather isn't it\",\n",
    "]\n",
    "\n",
    "football_sents_train = [\n",
    "    \"city looking good\",\n",
    "    \"advantage united\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words\n",
    "One of the simplying assumptions that is made with Naïve Bayes classification is that each document is taken to be a so-called bag of words. What this means is that the word order is ignored.  We are also going to implement the Bernouilli version of Naïve Bayes which means we are not taking into account the number of times a word appears in a document. \n",
    "\n",
    "Given these assumptions, we will represent each training document as a pair consisting of a dictionary that maps each word that appears in the document to `True`, and a string denoting the class of the document. \n",
    "\n",
    "### Exercise\n",
    "In the cell below, write code that achieves this. Create three lists:\n",
    "- `weather_data_train`: a list containing the data for documents in the class `weather`;\n",
    "- `football_data_train`: a list containing the data for documents in the class `football`;\n",
    "- `train_data`: which is simply `weather_data_train + football_data_train`\n",
    "\n",
    "**Hint**: this can be done with nested list comprehensions.  For the inner list comprehension, you create a list of (word, boolean) pairs for each document.  Duplicates can be ignored by calling dict() to convert this list to a dictionary.   **Alternatively**, just iterate over the sentences in the training set, build a dictionary for each sentence and then append it to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#uncomment the following line to load a solution\n",
    "#%load solutions/format_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How a  Naïve Bayes classifier works\n",
    "\n",
    "We will now look at how a NB work, using our `weather` versus `football` classification task.\n",
    "\n",
    "In order to classify a document, $d$, we need to determine which of these probabilities is greatest:\n",
    "\n",
    "$$P(\\,\\mbox{weather}\\,|\\,d) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad P(\\,\\mbox{football}\\,|\\,d)$$\n",
    "\n",
    "$d$ could, for example, be the string \"today is looking cloudy\", which would give us:\n",
    "\n",
    "$$P(\\,\\mbox{weather}\\,|\\,\\mbox{\"today is looking cloudy\"}) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad P(\\,\\mbox{football}\\,|\\,\\mbox{\"today is looking cloudy\"})$$\n",
    "\n",
    "The idea is that if the term on the left is higher then the document is in category `weather`, and if the term on the right is higher then the document is in category `football`.\n",
    "\n",
    "$P(X|Y)$ means the probability of $X$ given $Y$. So, $P(\\,\\mbox{weather}\\,|\\,d)$ means the probability, given a document $d$, of $d$ being of class `weather`.\n",
    "\n",
    "By applying Bayes' Rule (see the lecture notes), we can see that this is the same as comparing:\n",
    "\n",
    "$$P(\\,d\\,|\\,\\mbox{weather}\\,)\\cdot P(\\,\\mbox{weather}\\,) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad P(\\,d\\,|\\,\\mbox{football}\\,)\\cdot P(\\,\\mbox{football}\\,)$$\n",
    "\n",
    "Let's just look at what each of these probabilities mean?\n",
    "\n",
    "1. $P(\\,d\\,|\\,\\mbox{weather}\\,)$: this is the probability of a document in the `weather` category being the document $d$\n",
    "\n",
    "2. $P(\\,d\\,|\\,\\mbox{football}\\,)$: this is the probability of a document in the `football` category being the document $d$\n",
    "\n",
    "3. $P(\\,\\mbox{weather}\\,)$: this is the probability of a randomly selected document being of category `weather`.\n",
    "\n",
    "4. $P(\\,\\mbox{football}\\,)$: this is the probability of a randomly selected document being of category `football`.\n",
    "\n",
    "How are we going to obtain these probabilities? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class priors\n",
    "\n",
    "We have established that we need to know $P(\\,\\mbox{weather}\\,)$ and $P(\\,\\mbox{football}\\,)$. These are called the class priors. Let's see how we can obtain (estimated) values for these probabilities from our training data.\n",
    "\n",
    "The classifier has seen three documents of class `weather` and two documents of class `football`.\n",
    "\n",
    "From this it learns that `weather` documents are slightly more common that `football` documents, and it therefore has a slight bias towards saying a document is a `weather` document.\n",
    "\n",
    "To be more precise, the classifier has learned that:\n",
    "- The probability that a document is in class `weather` is $3/5$.  \n",
    "We say $P(\\mbox{weather})=3/5=0.6$.\n",
    "\n",
    "- The probability that a document is in class `football` is $2/5$.  \n",
    "We say $P(\\mbox{football})=2/5=0.4$.\n",
    "\n",
    "In general, if the training data contained $n_1$ documents of class `weather` and $n_2$ documents of class `football`, then \n",
    "\n",
    "$$P(\\mbox{weather})=\\frac{n_1}{n_1+n_2} \\qquad \\mbox{and} \\qquad P(\\mbox{football})=\\frac{n_2}{n_1+n_2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In blank the cell below, implement a function `class_priors(training_data)` that takes a dictionary of training data  and returns a dictionary that maps the name of each class to the class prior for that class.\n",
    "\n",
    "Once you have done this, test it out on the training data above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/class_priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional probabilities\n",
    "We now turn to the problem of how to calculate (estimates of) the probabilities such as $P(\\,d\\,|\\,\\mbox{weather}\\,)$ and $P(\\,d\\,|\\,\\mbox{football}\\,)$, for some document $d$. The problem we have is that $d$ is a document, potentially a long document, that we won't have seen in the training data.\n",
    "\n",
    "To address this, the Naïve Bayes model of document classification makes a major simplifying assumption.  In particular, it is assumed that the probabiity that different words occur in a document are independent of one another.\n",
    "\n",
    "For example, if $d=\\mbox{\"today is looking cloudy\"}$ then this assumption tells us that:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "P(\\,\\mbox{\"today is looking cloudy\"}\\,|\\,\\mbox{weather}\\,) &=& P(\\{\\mbox{\"today\"},\\mbox{\"is\"},\\mbox{\"looking\"},\\mbox{\"cloudy\"}\\}\\,|\\,\\mbox{weather}\\,)\\\\\n",
    "&=& P(\\,\\mbox{\"today\"}\\,|\\,\\mbox{weather}\\,)\\times P(\\mbox{\"is\"}\\,|\\,\\mbox{weather}\\,)\\times P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather}\\,)\\times P(\\mbox{\"cloudy\"}\\,|\\,\\mbox{weather}\\,)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "For the general case, with class $c$ and document $d=\\{w_1,\\ldots,w_n\\}$, we have:\n",
    "\\begin{eqnarray*}\n",
    "P(\\,d\\,|\\,c\\,) &=& P(\\,\\{w_1,\\ldots,w_n\\}\\,|\\,c\\,)\\\\\n",
    "&=& \\prod_{i=1}^n P(\\,w_i\\,|\\,c\\,)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "The point is that it is plausible that given a reasonable amount of training data, we can make reasonable estimates of the probabilities $P(\\,\\mbox{\"today\"}\\,|\\,\\mbox{weather}\\,)$, $P(\\mbox{\"is\"}\\,|\\,\\mbox{weather}\\,)$, $P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather}\\,)$, and  $P(\\mbox{\"cloudy\"}\\,|\\,\\mbox{weather}\\,)$. \n",
    "\n",
    "We now look at how that is done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating conditional probabilities\n",
    "So we have established that we need estimates of probabilities such as: $P(\\,\\mbox{\"cloudy\"}\\,|\\,\\mbox{weather}\\,)$, $P(\\mbox{\"is\"}\\,|\\,\\mbox{weather}\\,)$, $P(\\mbox{\"today\"}\\,|\\,\\mbox{weather}\\,)$, and $P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather}\\,)$.\n",
    "\n",
    "How can these probabilities be estimated from the training data?\n",
    "\n",
    "Look at the training data we set up above. There are 3 documents of class `weather` and within these documents there are a total of 11 tokens (8 different types).\n",
    "\n",
    "From the data we can estimate the following probabilities:\n",
    "- the probability of seeing \"today\" in a `weather` document is $\\frac{2}{11}$,  \n",
    "i.e. $P(\\mbox{\"today\"}\\,|\\,\\mbox{weather})=\\frac{2}{11}$;\n",
    "- the probability of seeing \"it\" in a `weather` document is $\\frac{2}{11}$,  \n",
    "i.e. $P(\\mbox{\"it\"}\\,|\\,\\mbox{weather})=\\frac{2}{11}$;\n",
    "- the probability of seeing \"is\" in a `weather` document is $\\frac{2}{11}$,  \n",
    "i.e. $P(\\mbox{\"is\"}\\,|\\,\\mbox{weather})=\\frac{2}{11}$;\n",
    "- the probability of seeing \"raining\" in a `weather` document is $\\frac{1}{11}$,  \n",
    "i.e. $P(\\mbox{\"raining\"}\\,|\\,\\mbox{weather})=\\frac{1}{11}$;\n",
    "- the probability of seeing \"looking\" in a `weather` document is $\\frac{1}{11}$,  \n",
    "i.e. $P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather})=\\frac{1}{11}$;\n",
    "- the probability of seeing \"cloudy\" in a `weather` document is $\\frac{1}{11}$,  \n",
    "i.e. $P(\\mbox{\"cloudy\"}\\,|\\,\\mbox{weather})=\\frac{1}{11}$;\n",
    "- the probability of seeing \"nice\" in a `weather` document is $\\frac{1}{11}$,  \n",
    "i.e. $P(\\mbox{\"nice\"}\\,|\\,\\mbox{weather})=\\frac{1}{11}$; and\n",
    "- the probability of seeing \"weather\" in a `weather` document is $\\frac{1}{11}$,  \n",
    "i.e. $P(\\mbox{\"weather\"}\\,|\\,\\mbox{weather})=\\frac{1}{11}$\n",
    "- the probability of seeing any other word in a `weather` document is 0.\n",
    "Notice that all of these conditional probabilities sum to $1$.\n",
    "\n",
    "We can do the same thing for the `football` documents:\n",
    "- the probability of seeing \"city\" in a `football` document is $\\frac{1}{5}$,  \n",
    "i.e. $P(\\mbox{\"city\"}\\,|\\,\\mbox{weather})=\\frac{1}{5}$;\n",
    "- the probability of seeing \"looking\" in a `football` document is $\\frac{1}{5}$,  \n",
    "i.e. $P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather})=\\frac{1}{5}$;\n",
    "- the probability of seeing \"good\" in a `football` document is $\\frac{1}{5}$,  \n",
    "i.e. $P(\\mbox{\"good\"}\\,|\\,\\mbox{weather})=\\frac{1}{5}$;\n",
    "- the probability of seeing \"advantage\" in a `football` document is $\\frac{1}{5}$,  \n",
    "i.e. $P(\\mbox{\"advantage\"}\\,|\\,\\mbox{weather})=\\frac{1}{5}$;\n",
    "- the probability of seeing \"united\" in a `football` document is $\\frac{1}{5}$,  \n",
    "i.e. $P(\\mbox{\"united\"}\\,|\\,\\mbox{weather})=\\frac{1}{5}$;\n",
    "- the probability of seeing any other word in a `football` document is 0.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "We now look at how to implement the calculation of conditional probabilties.\n",
    "\n",
    "In the empty cell below, define a function `cond_probs(training_data)` that takes training data and returns a dictionary that maps the name of a class, $c$, onto a dictionary that maps each word, $w$ to the conditional probability for that word given that class, i.e. $\\log(P(w|c))$.\n",
    "\n",
    "Hint: the following line will create a dictionary of the required form:  \n",
    "`c_probs = collections.defaultdict(lambda: defaultdict(float))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a paritial solution\n",
    "#%load solutions/cond_probs_partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/cond_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing the conditional probability of a document\n",
    "\n",
    "We have created implementations of the following functions:\n",
    "- `class_priors(training_data)` that computes estimates of the class priors from training data;\n",
    "- `cond_probs(training_data)` that computes estimates of the conditional probability of a word given a class from training data\n",
    "\n",
    "Let us suppose that we have applied these functions to our training data as follows.\n",
    "\n",
    "```\n",
    "c_priors = class_priors(train_data)\n",
    "c_probs = cond_probs(train_data)\n",
    "```\n",
    "\n",
    "`c_priors` and `c_probs` define the classifier.\n",
    "\n",
    "### Exercise\n",
    "In the cell below, complete the function `classify(doc,c_priors,c_probs)`. It should return the class that the classifier assigns to the document, `doc`. \n",
    "\n",
    "In the event of a tie, the function should randomly chose one of the classes (see `random.choice`). \n",
    "\n",
    "- Write your function in a way that allows for the possibilty of any number of classes.\n",
    "- Assume that the document, `doc`, is represented as a dictionary that maps words (in the document) to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(doc,priors,c_probs):\n",
    "\n",
    "    <put your definition of classify here>\n",
    "    \n",
    "\n",
    "c_priors = class_priors(train_data)\n",
    "c_probs = cond_probs(train_data)\n",
    "sent = \"looking cloudy today\"\n",
    "doc = dict([(word, True) for word in sent.split()])\n",
    "classify(doc,c_priors,c_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/my_NB_classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A problem\n",
    "There is a problem with the classifier that we have written. \n",
    "\n",
    "### Exercise\n",
    "Run the classifier on several examples to see if you can discover what the problem is. \n",
    "- You might need to run the same document through the classifier several times to see if you get different class for different runs\n",
    " - If you've implemented `classify` correctly then this happens when you have a tie.\n",
    "- Try the sentence \"city looking cloudy today\"\n",
    "\n",
    "What is going wrong?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add one smoothing\n",
    "It will often be the case that a word appears in documents of one class, but not in any documents of another class.\n",
    "- We saw that with the word \"city\" in the document \"city looking cloudy today\".\n",
    "- While \"city\" appears in documents of class `football`, it does not appear in any documents of class `weather`. \n",
    "- Thus, the conditional probabiity $P(\\,\\mbox{city}\\,|\\,\\mbox{weather}\\,)$ is equal to zero.\n",
    "- We are **multiplying** probabilities, so the document ends up with a score of zero even though all of the other words in the document suggest that it is of class `weather`.\n",
    "\n",
    "To get around this we need to do something called **smoothing** in order to avoid zero probabilities. \n",
    "\n",
    "In particular, we will implement a version of smoothing called **add-one smoothing**. This involves adding a count of one to all of the known vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Known vocabulary\n",
    "The words that appear in documents in the training data are collectively described as the known vocabulary. These are the words that the classifier can learn something about. If the classifier is asked to classify a document that contains any words that are not in the known vocabulary then the classifier will simply ignore them.\n",
    "\n",
    "### Exercise\n",
    "In the cell below, write a function `known_vocabulary(training_data)` that takes some training and returns a set containing all of words that appear in documents in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing add-one smoothing\n",
    "As the name suggests, add-one smoothing involves adding counts.\n",
    "\n",
    "In particular, for each word, $w$, in the known vocabulary and each class, $c$, we add one extra count to our record of how many times $w$ appears in documents of class $c$. We are, in effect, hallucinating counts. The reason for doing this is that it means that we avoid zero probabilties.\n",
    "\n",
    "### Exercise\n",
    "In the blank cell below copy in your code for the `cond_probs` function that you wrote earlier. Then adapt this code so that it implements the add-one smoothing scheme.\n",
    "- You will find it useful to use your `known_vocabulary` function.\n",
    "- If there are $k$ words in your known vocabulary, then you will add $k$ counts for each class. \n",
    "- Therefore, when calculating conditional probabilities, you need to add $k$ to the denominator to account for these extra counts.\n",
    "\n",
    "Test out your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/smoothed_cond_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignoring OOV words\n",
    "We now look at how to update the `classify` function that we wrote earlier so that it ignores out of vocabulary words that appear in a document being classified.\n",
    "\n",
    "### Exercise\n",
    "In the blank cell below, copy the `classify` method you wrote earlier and update it so that words not in the known vocabulary are ignored.\n",
    "- You will want to add an additional argument to the `classify` function that is a set containing the known vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/new_classify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension: Underflow\n",
    "\n",
    "We need to address one final problem concerning the multiplication of probabilities.\n",
    "\n",
    "Recall this equation from earlier:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "P(\\,d\\,|\\,c\\,) &=& P(\\,\\{w_1,\\ldots,w_n\\}\\,|\\,c\\,)\\\\\n",
    "&=& \\prod_{i=1}^n P(\\,w_i\\,|\\,c\\,)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "This tells us that in order to compute $P(\\,d\\,|\\,c\\,)$ for some document $d$ and class $c$, we must multiply $n$ conditional probabilities, one for each word in the document.\n",
    "\n",
    "While in our toy example, this is not an issue. However, in a more realistic settings, where we had thousands of documents, each of which contained multiple paragraphs, we would find ourselves multiplying large numbers of very small probabilities. This would lead to **underflow**.\n",
    "\n",
    "To avoid ths problem, we will add the log of probabilties. \n",
    "\n",
    "To understand why this is a reasonable thing to do let us recall a comparison from earlier:\n",
    "\n",
    "$$P(\\,d\\,|\\,\\mbox{weather}\\,)\\cdot P(\\,\\mbox{weather}\\,) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad P(\\,d\\,|\\,\\mbox{football}\\,)\\cdot P(\\,\\mbox{football}\\,)$$\n",
    "\n",
    "Our goal is to determine which of the values (on the left and right) is larger (or determine that they are equal). \n",
    "\n",
    "It should be clear that we will get exactly the same answer to this question by making the following comparsion.\n",
    "\n",
    "$$\\log(P(\\,d\\,|\\,\\mbox{weather}\\,)) + \\log(P(\\,\\mbox{weather}\\,)) \\qquad\\qquad \\mbox{versus} \\qquad\\qquad \\log(P(\\,d\\,|\\,\\mbox{football}\\,)) + \\log(P(\\,\\mbox{football}\\,))$$\n",
    "\n",
    "Thus, rather than calculating conditional probabilities as described above, we will calculate log conditional probabilities like this:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\log(P(\\,\\mbox{\"today is looking cloudy\"}\\,|\\,\\mbox{weather}\\,)) &=& \\log(P(\\{\\mbox{\"today\"},\\mbox{\"is\"},\\mbox{\"looking\"},\\mbox{\"cloudy\"}\\}\\,|\\,\\mbox{weather}\\,))\\\\\n",
    "&=& \\log(P(\\,\\mbox{\"today\"}\\,|\\,\\mbox{weather}\\,))\\ +\\\\\n",
    "&&\\log(P(\\mbox{\"is\"}\\,|\\,\\mbox{weather}\\,))\\ +\\\\\n",
    "&&\\log(P(\\mbox{\"looking\"}\\,|\\,\\mbox{weather}\\,))\\ + \\\\\n",
    "&&\\log(P(\\mbox{\"cloudy\"}\\,|\\,\\mbox{weather}\\,))\n",
    "\\end{eqnarray*}\n",
    "\n",
    "For the general case, with class $c$ and document $d=\\{w_1,\\ldots,w_n\\}$, we have:\n",
    "\n",
    "\\begin{eqnarray*}\n",
    "\\log(P(\\,d\\,|\\,c\\,)) &=& \\log(P(\\,\\{w_1,\\ldots,w_n\\}\\,|\\,c\\,))\\\\\n",
    "&=& \\sum_{i=1}^n \\log(P(\\,w_i\\,|\\,c\\,))\n",
    "\\end{eqnarray*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension Exercise\n",
    "In the blank cell below, make a copy of the cell containing the definition of `classify`.\n",
    "\n",
    "Adapt the code so that it adds logs of probabilties rather than multiplies probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/classify_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating a Naïve Bayes classifier on test data\n",
    "We are now ready to run our Naïve Bayes classifier on a set of test data. When we do this we want to return the accuracy of the classifier on that data, where accuracy is calculated as follows:\n",
    "\n",
    "$$\\frac{\\mbox{number of test documents that the classifier classifiers correctly}}\n",
    "{\\mbox{total number of test documents}}$$\n",
    "\n",
    "In order to compute this accuracy score, we need to give the classifier **labelled** test data.\n",
    "- This will be in the same format as the training data.\n",
    "\n",
    "### Exercise\n",
    "In the cell below, we set up 5 test documents in the class `weather` and 5 documents in the class `football`.\n",
    "\n",
    "Run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weather_sents_train = [\n",
    "    \"today it is raining\",\n",
    "    \"looking cloudy today\",\n",
    "    \"it is nice weather\",\n",
    "]\n",
    "\n",
    "football_sents_train = [\n",
    "    \"city looking good\",\n",
    "    \"advantage united\",\n",
    "]\n",
    "\n",
    "weather_data_train = [(dict([(word, True) for word in sent.split()]), \"weather\") for sent in weather_sents_train] \n",
    "football_data_train = [(dict([(word, True) for word in sent.split()]), \"football\") for sent in football_sents_train]\n",
    "train_data = weather_data_train + football_data_train\n",
    "\n",
    "weather_sents_test = [\n",
    "    \"the weather today is nice\",\n",
    "    \"it is raining cats and dogs\",\n",
    "    \"the weather here is wet\",\n",
    "    \"it was hot today\",\n",
    "    \"rain due tomorrow\",\n",
    "]\n",
    "\n",
    "football_sents_test = [\n",
    "    \"what a great goal that was\",\n",
    "    \"poor defending by the city center back\",\n",
    "    \"wow he missed a sitter\",\n",
    "    \"united are a shambles\",\n",
    "    \"shots raining down on the keeper\",\n",
    "]\n",
    "\n",
    "weather_data_test = [(dict([(word, True) for word in sent.split()]), \"weather\") for sent in weather_sents_test] \n",
    "football_data_test = [(dict([(word, True) for word in sent.split()]), \"football\") for sent in football_sents_test]\n",
    "test_data = weather_data_test + football_data_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the cell below Implement a `NB_evaluate` function that returns the accuracy of a classifier on a set of labelled test data.\n",
    "`NB_evaluate` should make use of the `classify` function, and take the following arguments:\n",
    "- the test data\n",
    "- the class priors\n",
    "- the conditional probabilities\n",
    "- the known vocabulary (though this is redundant since it could be computed from the conditional probabilities)\n",
    "\n",
    "`NB_evaluate` should return the accuracy of the classifier on the test data.\n",
    "\n",
    "Try out your `NB_evaluate` function on the test data in the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a full solution\n",
    "#%load solutions/NB_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Try out the classifier on different training and test examples.\n",
    "\n",
    "Before running each test try to anticipate what you think the outcome will be.\n",
    "- What happens when none of the words in the test document are in the known vocabulary?\n",
    "- Can you set up data so that there is a tie between the classes, and the classifier randomly chooses a class. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
