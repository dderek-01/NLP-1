{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 4: Part-of-speech (PoS) Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminaries \n",
    "Run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b9e1b9f5104b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'/Users/warrenboult/Documents/MSC/nlp/resources'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(r'/Users/warrenboult/Documents/MSC/nlp/resources')\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from collections import defaultdict,Counter\n",
    "from itertools import zip_longest\n",
    "from IPython.display import display\n",
    "from random import seed\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pylab as pylab\n",
    "%matplotlib inline\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'large',\n",
    "         'axes.titlesize':'large',\n",
    "         'xtick.labelsize':'large',\n",
    "         'ytick.labelsize':'large'}\n",
    "pylab.rcParams.update(params)\n",
    "from pylab import rcParams\n",
    "from operator import itemgetter, attrgetter, methodcaller\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokens vs Types\n",
    "This session concerns the task of part-of-speech tagging. It is loosely divided into 2 parts: the first part deals with the notion of PoS ambiguity of a vocabulary type; and the second part compares the performance of two taggers on various corpora.\n",
    "\n",
    "We will be making an important distinction between tokens and types. A sentence in a document is make up of a sequence of tokens. For example the list\n",
    "`[\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\"]`  \n",
    "contains 7 tokens, but only 6 distinct strings: there are two occurrences of `\"the\"`. \n",
    "\n",
    "The way we say this is that there are 6 **types** in the sentence, but 7 **tokens**. Tokens are occurrences of types.\n",
    "\n",
    "In this session we will be looking at the ambiguity of types not tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average PoS tag ambiguity \n",
    "The Part-of-Speech (PoS) tag ambiguity of a type is a measure of how varied the PoS tags are for that type. \n",
    "\n",
    "Some types are always (or almost always) labelled with the same PoS tag, so exhibit no (or very little) ambiguity. It is easy to predict the correct PoS tag for such words. \n",
    "\n",
    "On the other hand, a type that is commonly labelled by a variety of different PoS tags exhibits a high level of ambiguity, and is more challenging to deal with.\n",
    "\n",
    "In this session, we are going to be considering two measures of a type's ambiguity. \n",
    "\n",
    "In this section, we consider a simple measure that just counts the number of different tags that label the type. \n",
    "\n",
    "In the next section we will look at a more complex information-theoretic measure based on entropy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the blank cell below, create a function `simple_pos_ambiguity`. \n",
    "\n",
    "Here is the docstring for `simple_pos_ambiguity`:\n",
    "```\n",
    "    \"\"\"\n",
    "    for each type in the Walls Street Journal corpus, this \n",
    "    function determines the number of different PoS tags that\n",
    "    the type as been assigned.\n",
    "\n",
    "    :param none\n",
    "    :return: A dictionary (hashmap) mapping each type to its \n",
    "            degree of ambiguity (the number of distinct PoS tags \n",
    "            that the type is labelled with in the Wall Street \n",
    "            Journal Corpus).\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "Create `simple_pos_ambiguity` as follows:\n",
    "\n",
    "1. Create a Wall Street Journal corpus reader\n",
    "2. Use the corpus reader's method `tagged_words`, to get a list of all tokens in the corpus tagged with their PoS (e.g. if your corpus reader is called `wsj_reader`, then you'd call `wsj_reader.tagged_words()`). This method is available because the Wall Street Journal corpus has been hand-annotated with PoS tags.\n",
    "3. For each type, build a set containing all of the different PoS tags that are assigned to that type. So if in the Wall Street Journal corpus \"red\" occurred only as a noun and adjective, then this number would be a two element set containing just these two part-of-speech tags. The size (cardinality) of the set is the ambiguity of that type. See below for details.\n",
    "4. Return a Python dictionary (hashmap) mapping each type to its ambiguity.  \n",
    "\n",
    "Some useful hints:\n",
    "- It will be useful to have this line: `from collections import defaultdict`.\n",
    "- See https://docs.python.org/3/library/collections.html#collections.defaultdict for how to use `defaultdict`.\n",
    "- Think carefully about what is an appropirate type to give `defaultdict` as a parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Sussex NLTK root directory is', '/Users/warrenboult/Documents/MSC/nlp/resources')\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'encoding' is an invalid keyword argument for this function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-402dcfd14e0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msussex_nltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_readers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWSJCorpusReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mwsj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWSJCorpusReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwsj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/warrenboult/Documents/MSC/nlp/resources/sussex_nltk/corpus_readers.pyc\u001b[0m in \u001b[0;36mtagged_words\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfileid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspaths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'._'\u001b[0m \u001b[0;32mnot\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0mfileid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UTF8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'encoding' is an invalid keyword argument for this function"
     ]
    }
   ],
   "source": [
    "from sussex_nltk.corpus_readers import WSJCorpusReader\n",
    "from collections import defaultdict\n",
    "wsj = WSJCorpusReader()\n",
    "for tok,tag in wsj.tagged_words():\n",
    "    print tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load solutions/simple_ambiguity\n",
    "from collections import defaultdict\n",
    "from sussex_nltk.corpus_readers import WSJCorpusReader\n",
    "\n",
    "def simple_pos_ambiguity():\n",
    "    \"\"\"\n",
    "    for each type in the Walls Street Journal corpus, this \n",
    "    function determines the number of different PoS tags that\n",
    "    the type as been assigned.\n",
    "\n",
    "    :param none\n",
    "    :return: A dictionary (hashmap) mapping each type to its \n",
    "            degree of ambiguity (the number of distinct PoS tags \n",
    "            that the type is labelled with in the Wall Street \n",
    "            Journal Corpus).\n",
    "    \"\"\"\n",
    "    wsj_reader = WSJCorpusReader()    #Create a new reader\n",
    "    tags_dict = defaultdict(set)\n",
    "    for tok,tag in wsj_reader.tagged_words():\n",
    "        tags_dict[tok].add(tag)\n",
    "    count_dict = defaultdict(int)\n",
    "    for ty in tags_dict.keys():\n",
    "        count_dict[ty] = len(tags_dict[ty])\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the blank cell below, check that the ambiguity of \"*blue*\" is 2 in the Wall Street Journal corpus. It occurs as a noun and adjective only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'encoding' is an invalid keyword argument for this function",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-487b86dcc759>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# %load solutions/blue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msimple_pos_ambiguity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"blue\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-457adae01826>\u001b[0m in \u001b[0;36msimple_pos_ambiguity\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mwsj_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWSJCorpusReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m#Create a new reader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtags_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtok\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwsj_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagged_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mtags_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mcount_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/warrenboult/Documents/MSC/nlp/resources/sussex_nltk/corpus_readers.pyc\u001b[0m in \u001b[0;36mtagged_words\u001b[0;34m(self, fileids)\u001b[0m\n\u001b[1;32m    590\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfileid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspaths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'._'\u001b[0m \u001b[0;32mnot\u001b[0m  \u001b[0;32min\u001b[0m \u001b[0mfileid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 592\u001b[0;31m                 \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfileid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'UTF8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    593\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'encoding' is an invalid keyword argument for this function"
     ]
    }
   ],
   "source": [
    "# %load solutions/blue\n",
    "dict = simple_pos_ambiguity()\n",
    "dict[\"blue\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the blank cell below, write code to find the average ambituity of words in the Wall Street Journal corpus.\n",
    "\n",
    "This might be useful:  \n",
    "`from scipy import mean`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "#%load solutions/average_ambiguity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy as a measure of ambiguity \n",
    "\n",
    "In this activity, you are given a function that calculates PoS ambiguity in a different way, using the notion of [entropy](http://en.wikipedia.org/wiki/Entropy_(information_theory). \n",
    "\n",
    "Below we will find a function `get_entropy_ambiguity` that is used to get a measure of the PoS ambiguity of a word in the Wall Street Journal corpus based on entropy.\n",
    "\n",
    "First let's get a sense of how entropy works.\n",
    "\n",
    "Entropy is a measure of uncertainty. A word will have high entropy when it occurs the same number of times with each part of speech. There is maximum uncertainty as to which part of speech it has.\n",
    "\n",
    "The larger the part of speech tagset, the greater the potential for uncertainty, and the higher the entropy can be.\n",
    "\n",
    "### Exercise\n",
    "In the cell below we see a function `entropy`. It's argument is a list of counts (which in our case are counts of how many times a word appeared with a given part of speech).\n",
    "\n",
    "Check that you understand how the code implements this definition of entropy:\n",
    "$$H([x_1,\\ldots,x_n])=\\sum_{i=1}^nP(x_i)\\log_2 P(x_i)$$\n",
    "where $n$ is the number of PoS tags, and $x_i$ is a count of how many times the word was labelled with the $i$th PoS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def entropy(counts):            # counts = list of counts of occurrences of tags\n",
    "    total = sum(counts)         # get total number of occurrences\n",
    "    if not total: return 0      # if zero occurrences in total, then 0 entropy\n",
    "    entropy = 0\n",
    "    for i in counts:            # for each tag count\n",
    "        p = i/total      # probability that the token occurs with this tag\n",
    "        try:\n",
    "            entropy += p * log(p,2) # add to entropy\n",
    "        except ValueError: pass     # if p==0, then ignore this p\n",
    "    return -entropy if entropy else entropy   # only negate if nonzero, otherwise \n",
    "                                              # floats can return -0.0, which is weird.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the empty cell below, experiment with the `entropy` function.\n",
    "- It takes a list of counts as its argument.\n",
    "- Compare the entropy of a list where all counts are the same with the entropy of a list of different counts.\n",
    "- Investigate the effect of varying the length of the list of counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to look at the `get_entropy_ambiguity` function.\n",
    "\n",
    "Although it isn't efficient, in order to keep the code simple, `get_entropy_ambiguity` only computes the ambiguity of one word for any given call. This means that to find the average entropy of all of the types in the corpus, you would have to call the function once per type.\n",
    "\n",
    "### Exercise\n",
    "Have a careful look at the code for `get_entropy_ambiguity` in the cell below.\n",
    "\n",
    "Note that the code below uses `try-except` statements. The code under the try statement is executed, and if an exception is raised, then the code under the except statement is executed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "from sussex_nltk.corpus_readers import WSJCorpusReader\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_entropy_ambiguity(word):\n",
    "# Get the PoS ambiguity of *word* according to its occurrence in WSJ\n",
    "    pos_counts = defaultdict(int)       # keep track of the number of times *word* \n",
    "                                        # appears with each PoS tag\n",
    "    for token, tag in WSJCorpusReader().tagged_words():   \n",
    "        if token == word:               \n",
    "            pos_counts[tag] += 1\n",
    "    return entropy(pos_counts.values()) \n",
    "\n",
    "def entropy(counts):            # counts = list of counts of occurrences of tags\n",
    "    total = sum(counts)         # get total number of occurrences\n",
    "    if not total: return 0      # if zero occurrences in total, then 0 entropy\n",
    "    entropy = 0\n",
    "    for i in counts:            # for each tag count\n",
    "        p = i/total      # probability that the token occurs with this tag\n",
    "        try:\n",
    "            entropy += p * log(p,2) # add to entropy\n",
    "        except ValueError: pass     # if p==0, then ignore this p\n",
    "    return -entropy if entropy else entropy   # only negate if nonzero, otherwise \n",
    "                                              # floats can return -0.0, which is weird.\n",
    "    \n",
    "# Usage:\n",
    "print('Ambiguity of \"blue\": {0:.4f}'.format(get_entropy_ambiguity(\"blue\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "- Use your simple measure of PoS ambiguity (from the previous section) to calculate the PoS ambiguity of the words \"*either*\" and \"*value*\". \n",
    "- Now do the same with the entropy-based ambiguity measure. \n",
    "- How do the measures differ? \n",
    "- Which measure produces a more representative figure for how ambiguous the PoS of a type is?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with PoS taggers\n",
    "In this section you will have a chance to use two different Part-of-Speech taggers: the NLTK Maximum Entropy PoS tagger; and the Twitter-specific PoS tagger from Gimpel et al.\n",
    "\n",
    "The following code shows you how to use these taggers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named sussex_nltk.corpus_readers",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-----------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-04a6dee9677b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msussex_nltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus_readers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReutersCorpusReader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msussex_nltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtwitter_tag_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named sussex_nltk.corpus_readers"
     ]
    }
   ],
   "source": [
    "from sussex_nltk.corpus_readers import ReutersCorpusReader\n",
    "from sussex_nltk.tag import twitter_tag_batch\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "number_of_sentences = 10     #Number of sentences to sample and display\n",
    "rcr = ReutersCorpusReader()  #Create a corpus reader\n",
    "sentences = rcr.sample_raw_sents(number_of_sentences)  #Sample some sentences\n",
    "\n",
    "#Tag with twitter specific tagger\n",
    "# - it also tokenises for you in a twitter specific way\n",
    "twitter_tagged = twitter_tag_batch(sentences)   \n",
    "\n",
    "#Tag with NLTK's maximum entropy tagger         \n",
    "nltk_tagged = [pos_tag(word_tokenize(sentence)) for sentence in sentences] \n",
    "\n",
    "#Print results for each sentence\n",
    "for raw, twitter_sentence, nltk_sentence in zip(sentences,twitter_tagged,nltk_tagged):\n",
    "    print(\"\\n\",raw,\"\\n\")\n",
    "    df = pd.DataFrame(list(zip_longest([(token,tag) for token,tag in nltk_sentence],\n",
    "                                       [(token,tag) for token,tag in twitter_sentence])),\n",
    "                      columns=[\"nltk tagger\",\"twitter tagger\"])\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Make a copy of the cell above that ran the two taggers on a sample of Reuters data, and move the copy to be positioned below this cell.\n",
    "\n",
    "Adapt the code so that it runs both taggers on a sample of sentences from the Reuters, Medline and Twitter corpora.\n",
    "\n",
    "Then run the code and try to observe limitations and strengths of the taggers on the various corpora.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "#%load solutions/tag_all_corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extension: Using PoS feature for classification\n",
    "In the blank cell below, investigate the performance of the Naïve Bayes classifier (developed for reviews in previous labs) with two different feature extraction functions involving PoS information:\n",
    "- A feature extraction function that returns just the PoS tags, i.e. no token.\n",
    "- A feature extraction function that returns a new token that results from concatenating the token and its PoS.\n",
    "\n",
    "How do these compare to the standard setup where no feature extractor is used?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#uncomment the line below to load a solution\n",
    "# %load solutions/classification_with_PoS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
