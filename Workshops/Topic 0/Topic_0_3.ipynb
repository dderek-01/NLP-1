{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic 0: Introduction to Python (Part 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the third part of the Introduction to Python for Natural Language Engineering course.\n",
    "\n",
    "These notebooks are designed to give you the working knowledge of Python necessary to complete the lab sessions for Natural Language Engineering. \n",
    "\n",
    "From the first 2 notebooks you should be familiar with a range of data types including strings, lists, sets, tuples and dictionaries.  You should also be familiar with defining your own functions as well as a number of built-in functions including print(), type(), range() and zip().  This notebook will introduce a number of more complex features including list comprehensions, map(), lazy generators and running python programs in other environments.  It will also introduce two Python libraries - Collections and Pandas.\n",
    "\n",
    "As in the last session:-\n",
    "\n",
    "- Run all of the code cells as you work through the notebook. \n",
    "- Try to understand what is happening in each code cell and predict the output before running it.\n",
    "- Complete all of the exercises.\n",
    "- Solutions to all exercises are provided, but please avoid loading the solution until you have had a go at solving it yourself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following cell twice, first to load some set up code, then again to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name zip_longest",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e8c499fcf016>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mzip_longest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name zip_longest"
     ]
    }
   ],
   "source": [
    "# %load ../setup\n",
    "import sys\n",
    "#sys.path.append(r'T:\\Departments\\Informatics\\LanguageEngineering') \n",
    "sys.path.append(r'/Users/davidw/Documents/teach/NLE/resources')\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from collections import defaultdict,Counter\n",
    "from itertools import zip_longest\n",
    "from IPython.display import display\n",
    "from random import seed\n",
    "get_ipython().magic('matplotlib inline')\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'large',\n",
    "         'axes.titlesize':'large',\n",
    "         'xtick.labelsize':'large',\n",
    "         'ytick.labelsize':'large'}\n",
    "pylab.rcParams.update(params)\n",
    "from pylab import rcParams\n",
    "from operator import itemgetter, attrgetter, methodcaller\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anyone who has previously programmed in Java will be familiar with the concept of objects.  A Python class  is a complex type whichallows the encapsulation of attributes and methods.  You have already been using a number of Python classes (e.g., strings, lists, dictionaries).  However, sometimes it is useful to be able to define new classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Student:\n",
    "    passmark=50  #this is a class variable which will be shared by all instances of Student\n",
    "    \n",
    "    def __init__(self,name,mark):  \n",
    "        \"\"\"\n",
    "        initialisation method run when a new instance is created\n",
    "        in general it can take any number of arguments (in addition to self)\n",
    "        :param self: this instance, name: name of Student, mark: mark of Student\n",
    "        \"\"\"\n",
    "        self.name=name  #store the name in an instance variable called name\n",
    "        self.mark=mark  #store the mark in an instance variable called mark\n",
    "        \n",
    "    def passes(self):\n",
    "        \"\"\"\n",
    "        has this student passed the course?\n",
    "        check whether the mark associated with this instance is greater than the class variable Passmark\n",
    "        :param self: this instance\n",
    "        :returns boolean\n",
    "        \"\"\"\n",
    "        return self.mark > Student.passmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(Student)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an instance of a class (remember every class defines a type)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "student1 = Student(\"Jack\",40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "student1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(student1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "student1.passes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Default Dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we import the collections library, we can make dictionaries that have a default value when none is specified.  This avoids having to use .get() every time we perform a dictionary lookup of a key which may not be present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To do this, we need to use a class that is not built-in, so we import it\n",
    "import collections\n",
    "word_counts = collections.defaultdict(int)\n",
    "# the \"int\" parameter will create entries with a default value of 0\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"This\" in word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_counts[\"This\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# an entry has been automatically created with the default value of 0, just by querying the default dictionary\n",
    "\"This\" in word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can add a new entry with a value of 1\n",
    "word_counts[\"is\"] += 1\n",
    "#querying this key in the default dictionary makes an entry with the default value of 0, and we add 1 to this\n",
    "word_counts[\"is\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we can also update the value of a key\n",
    "word_counts[\"is\"] += 5\n",
    "word_counts[\"is\"]\n",
    "6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The map function\n",
    "This takes a function and an iterable (e.g. a list) as arguments. It then applies the function to every item of the iterable, returning a list of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First we make a function, which we will pass to the map function in the next cell\n",
    "natural_numbers = range(5)\n",
    "def square(n):\n",
    "    return n**2\n",
    "\n",
    "square(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "squared_numbers = map(square, natural_numbers)\n",
    "for i in squared_numbers:\n",
    "    print (i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def decorate(char):\n",
    "     return \"*\" + char + \"*\"\n",
    "\n",
    "decorate(\"A\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decorated_characters = map(decorate, \"Hello\")\n",
    "type(decorated_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "decorated_characters = map(decorate, \"Hello\")\n",
    "for char in (decorated_characters):\n",
    "     print (char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the blank cell below write a function called `add_exclamation` which adds a `'!'` to the input string. Then map add_exclamation to print each word in dickens_words, followed by an exclamation point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "# %load solutions/add_exclamations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the next code cell we see code that determines the kinds of tokens found in a list. A token is a specific occurrence of a basic unit of lexical processing, typically a word or an item of punctuation.\n",
    "\n",
    "- Study the programme, in particular the string methods. These are very useful in NLP.\n",
    "- Experiment with the string methods using the empty cell until you understand how they work in special cases such as a single space and a single punctuation mark.\n",
    "- The programme will only assign one feature to each token. Are there any cases where more than one feature should be assigned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This', 'alpha')\n",
      "('text', 'alpha')\n",
      "('is', 'alpha')\n",
      "('100', 'digit')\n",
      "('times', 'alpha')\n",
      "('more', 'alpha')\n",
      "('pointless', 'alpha')\n",
      "('than,', 'other')\n",
      "('but', 'alpha')\n",
      "('no', 'alpha')\n",
      "('more,', 'other')\n",
      "('a', 'alpha')\n",
      "('new', 'alpha')\n",
      "('paragraph./n', 'other')\n",
      "('Why?', 'other')\n"
     ]
    }
   ],
   "source": [
    "def make_tokens(input_text):\n",
    "    \"\"\"\n",
    "    Take an input text, split it into tokens, find the\n",
    "    token's shape, make a feature\n",
    "    vector with the token itself and its shape, return\n",
    "    a list of all token feature vectors found in the input.\n",
    "    :param input_text: A character string containing spaces\n",
    "    :return: A list of token feature vectors (token, shape).\n",
    "        Sample output: [('a', 'alpha'), ('7', 'digit'), ('A27', 'alnum')]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Here we define a token as being delimited by a whitespace:\n",
    "    \n",
    "    tokens = input_text.split()\n",
    "    return map(make_token_feature_vector, tokens)\n",
    "\n",
    "\n",
    "def make_token_feature_vector(token):\n",
    "    \"\"\"\n",
    "    Given a token, extract its shape and return a\n",
    "    vector with the token itself and its shape\n",
    "    :param token: A character string\n",
    "    :return: A tuple (token, shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    if token.isalpha():\n",
    "        return (token, \"alpha\")\n",
    "    elif token.isdigit():\n",
    "        return (token, \"digit\")\n",
    "    elif token.isalnum():\n",
    "        return (token, \"alnum\")\n",
    "    elif token in \",:;\":  \n",
    "        return (token, \"punctuation\")\n",
    "    elif token in \".!?\":  \n",
    "        return (token, \"sentence_end\")\n",
    "    elif token == \"\\n\":  \n",
    "        return (token, \"paragraph_end\")\n",
    "    else:\n",
    "        return (token, \"other\")\n",
    "\n",
    "\n",
    "sample_text = 'This text is 100 times more pointless than, but no more, a new paragraph./n Why?'\n",
    "for token in make_tokens(sample_text):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List comprehension\n",
    "\n",
    "List comprehensions can be used to create a list of squares.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[x**2 for x in range (4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squares = [x*x for x in range(4)]\n",
    "type(squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(squares)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List comprehensions can be used to create a list of decorated characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['*H*', '*e*', '*l*', '*l*', '*o*']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\"*\" + char + \"*\" for char in \"Hello\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List comprehensions can be used to create a list of even numbers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[double(n) for n in range(4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function, `is_even` returns `True` for even numbers, and `False`, otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remember the mod operator % returns the residue after integer division\n",
    "def is_even(n):\n",
    "    return not n % 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_even(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_even(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List comprehensions can be used with our `is_even` function to create a list of squares for the first even numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 16, 36, 64, 100, 144, 196]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[square(n) for n in range(15) if is_even(n)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the blank cell below create a list of the odd numbers in the range 0-20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in range(20) if (x % 2 != 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/odd_list\n",
    "[n for n in range(20) if not is_even(n)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "In the blank cell below create a list of numbers in the range 0-20 that are both odd AND divisible by 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 9, 15]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x for x in range(20) if (x % 3 == 0) and not is_even(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load solutions/odd_div_by_three\n",
    "[n for n in range(20) if not is_even(n) and not n % 3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now take one last look at the code to that counts the number of sentences in each paragraph of a text.\n",
    "\n",
    "This version uses `map` to iterate over a list. The advantage of this is that it is no longer necessary to initialize the list and append elements to it in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paragraph 0 contains 2 sentence(s)\n"
     ]
    }
   ],
   "source": [
    "def count_sentences_per_paragraph(input_text):\n",
    "    \"\"\"\n",
    "    Given an input text:\n",
    "     - assign a number to each paragraph,\n",
    "     - count the number of sentences in each paragraph,\n",
    "     - output a list of all paragraph numbers together\n",
    "       with the number of sentences in it.\n",
    "\n",
    "    :param input_text: A character string possibly containing\n",
    "                        periods \".\" to separate sentences and\n",
    "                        paragraph marks \"\\n\" to separate\n",
    "                        paragraphs.\n",
    "    :return: A list of ordered pairs (tuples) where the first\n",
    "            element of the pair is the paragraph number and\n",
    "            the second element is the number of sentences in\n",
    "            that paragraph.\n",
    "            Sample output: [(0, 1), (1, 3), (2, 3), (3, 1)]\n",
    "    \"\"\"\n",
    "    \n",
    "    paragraphs = input_text.split(\"\\n\")\n",
    "    \n",
    "    # Apply the count_sentences function to every element of paragraphs,\n",
    "    # return the results in a new list, call it sentence_counts:\n",
    "    \n",
    "    sentence_counts = map(count_sentences, paragraphs)\n",
    "    paragraph_numbers = range(len(paragraphs))\n",
    "    sentences_per_paragraph = zip(paragraph_numbers, sentence_counts)\n",
    "    return sentences_per_paragraph\n",
    "\n",
    "def count_sentences(paragraph):\n",
    "    \"\"\"\n",
    "    A sentence is a character string delimited by a period \".\"\n",
    "    Given an input paragraph, return the number of sentences\n",
    "    in it.\n",
    "    :param paragraph: Character string with sentences.\n",
    "    :return: number of sentences in the input paragraph\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = paragraph.split(\".\")\n",
    "    return len(sentences)\n",
    "\n",
    "for para, count in count_sentences_per_paragraph(sample_text):\n",
    "    print(\"paragraph {0} contains {1} sentence(s)\".format(para,count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While, in the above code, the use of the map function is considered acceptable python style, it can be used to produce more complicated code which is difficult to read and would be considered poor style. In such cases it is considered good practice to use list comprehensions.\n",
    "\n",
    "### Exercise\n",
    "Make a copy of the code cell above and move it to be below this cell. Then adapt the code to use a list comprehension instead of `map`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paragraph 0 contains 2 sentence(s)\n"
     ]
    }
   ],
   "source": [
    "def count_sentences_per_paragraph(input_text):\n",
    "    \"\"\"\n",
    "    Given an input text:\n",
    "     - assign a number to each paragraph,\n",
    "     - count the number of sentences in each paragraph,\n",
    "     - output a list of all paragraph numbers together\n",
    "       with the number of sentences in it.\n",
    "\n",
    "    :param input_text: A character string possibly containing\n",
    "                        periods \".\" to separate sentences and\n",
    "                        paragraph marks \"\\n\" to separate\n",
    "                        paragraphs.\n",
    "    :return: A list of ordered pairs (tuples) where the first\n",
    "            element of the pair is the paragraph number and\n",
    "            the second element is the number of sentences in\n",
    "            that paragraph.\n",
    "            Sample output: [(0, 1), (1, 3), (2, 3), (3, 1)]\n",
    "    \"\"\"\n",
    "    \n",
    "    paragraphs = input_text.split(\"\\n\")\n",
    "    \n",
    "    # Apply the count_sentences function to every element of paragraphs,\n",
    "    # return the results in a new list, call it sentence_counts:\n",
    "    \n",
    "    sentence_counts = [count_sentences(para) for para in paragraphs]\n",
    "    paragraph_numbers = range(len(paragraphs))\n",
    "    sentences_per_paragraph = zip(paragraph_numbers, sentence_counts)\n",
    "    return sentences_per_paragraph\n",
    "\n",
    "def count_sentences(paragraph):\n",
    "    \"\"\"\n",
    "    A sentence is a character string delimited by a period \".\"\n",
    "    Given an input paragraph, return the number of sentences\n",
    "    in it.\n",
    "    :param paragraph: Character string with sentences.\n",
    "    :return: number of sentences in the input paragraph\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = paragraph.split(\".\")\n",
    "    return len(sentences)\n",
    "\n",
    "for para, count in count_sentences_per_paragraph(sample_text):\n",
    "    print(\"paragraph {0} contains {1} sentence(s)\".format(para,count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# uncomment the next line and then run the cell to load a solution\n",
    "# %load solutions/count_sentences_per_paragraph_list_comp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "There is a problem with the code in the cell above. The same problem exists in all our versions of this programme. \n",
    "\n",
    "- First look at the code and see if you can see where the problem lies. \n",
    "- Next, if you haven't found it, do some experimenting with the split function. Do you really understand how it works?\n",
    "- Try loading the file `sample_corpus_2.txt` and running the programme on it. It is best to carry out a separate experiment in a new cell.\n",
    "- Study the input and output until you understand what the problem was. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy generators\n",
    "We now introduce lazy generators, an important form of function in python. A lazy generator does not calculate its results all at once, but returns them one a a time for iteration. The `enumerate` function which we saw earlier is a lazy generator.\n",
    "\n",
    "You can define lazy generator functions by using `yield` instead of `return`. When the function reaches a `yield` command it yields the argument and suspends execution without terminating and returns control to the level that called the function. The next time it is called it it resumes from the same place that it was left. There is no requirement to have a single yield command. You can yield in one place the first time and another place the next time.\n",
    "\n",
    "The cell below shows a simple function using both forms so that you can see the difference. Notice that you cannot use the result in the same way. A result that is returned is passed directly as value whereas a result that is yielded must be used in an iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "yield\n",
      "<generator object yield_count_to_ten at 0x11297fc30>\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "def return_count_to_ten():\n",
    "    return range(1,11)\n",
    "\n",
    "\n",
    "def yield_count_to_ten():\n",
    "    for i in range(1, 11):\n",
    "        yield i\n",
    "\n",
    "        \n",
    "l = return_count_to_ten()\n",
    "print(l)\n",
    "    \n",
    "i = yield_count_to_ten()\n",
    "print ('yield')\n",
    "print(i)\n",
    "\n",
    "l = list(yield_count_to_ten())\n",
    "print(l)\n",
    "\n",
    "for i in yield_count_to_ten():\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous programme delimited tokens by looking for spaces between them. You should have noticed that it doesn't work very well because it doesn't account for punctuation symbols. We need a better way to do this and, ideally, a separate function to do it.\n",
    "\n",
    "Because it is hard to follow, here is a summary of the logic of the new function, `split_tokens(input_text)`:\n",
    "\n",
    "The function reads the whole string one character at a time, adding characters to the token variable.\n",
    "- When it encounters a delimiter it yields the token.\n",
    "- If the token is empty it yields the delimiter character - unless it is a space - because the delimiter is an item of punctuation which is itself a token.\n",
    "- After returning a token the variable is reset to an empty string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'alpha')\n",
      "('blind', 'alpha')\n",
      "('mouse', 'alpha')\n",
      "('run', 'alpha')\n",
      "('down', 'alpha')\n",
      "('3', 'digit')\n",
      "('stairs', 'alpha')\n",
      "('?', 'sentence_end')\n"
     ]
    }
   ],
   "source": [
    "def make_tokens(input_text):\n",
    "    \"\"\"\n",
    "    Take an input text, split it into tokens, find the\n",
    "    token's shape, make a feature\n",
    "    vector with the token itself and its shape, return\n",
    "    a list of all token feature vectors found in the input.\n",
    "    :param input_text: A character string containing spaces\n",
    "    :return: A list of token feature vectors (token, shape).\n",
    "        Sample output: [('a', 'alpha'), ('7', 'digit'), ('A27', 'alnum')]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Now it's up to the split_tokes function to decide what a token is.\n",
    "    # List comprehension creates a list by extracting elements from\n",
    "    # an iterable object, in this case Python automatically converts the\n",
    "    # split_tokens function into an iterable object because it uses the \"yield\" statement:\n",
    "    \n",
    "    tokens = [token for token in split_tokens(input_text)]\n",
    "    return map(make_token_feature_vector, tokens)\n",
    "\n",
    "\n",
    "def split_tokens(input_text):\n",
    "    \"\"\"\n",
    "    This function decides how to delimit a token. It takes an input\n",
    "    string, iterates over it character by character; it collects\n",
    "    constituent characters in the output token; punctuation characters\n",
    "    are considered delimiters therefore become tokens of their own; the\n",
    "    space character is removed from tokens. Yield each found token at\n",
    "    a time.\n",
    "    :param input_text: A character string containing a mix of text and delimiter characters.\n",
    "    :yield A character string which is either free from delimiters or\n",
    "        is a delimiter itself.\n",
    "    \"\"\"\n",
    "\n",
    "    DELIMITERS = \",:!?.\\n\"\n",
    "    token = \"\"\n",
    "    for char in input_text:\n",
    "        if char in DELIMITERS:  # test if the input character is a delimiter (substring presence)\n",
    "            \n",
    "            # Character strings, lists, etc, have a logical truth value in Python;\n",
    "            # an empty string is False, if it has characters it is True.\n",
    "            \n",
    "            if not token:  # same as token == \"\"\n",
    "                yield char\n",
    "            else:\n",
    "                \n",
    "                # Return token to the calling program, but next time this function\n",
    "                # is called, continue from\n",
    "                # the next statement rather than from the beginning of the function:\n",
    "                \n",
    "                yield token  # After yielding control to the calling program,\n",
    "                             # this function will execute the next statement:\n",
    "                token = \"\"  # Pick up execution from here.\n",
    "                yield char\n",
    "        elif char == \" \":\n",
    "            if token:  # same as token != \"\"\n",
    "                yield token\n",
    "                token = \"\"\n",
    "        else:\n",
    "            token += char\n",
    "\n",
    "sample_text = 'The blind mouse run down 3 stairs?'\n",
    "for token in make_tokens(sample_text):\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the function `split_tokens` yields the result instead of returning it. This means that it continues from the same point next time it is called.\n",
    "\n",
    "### Exercise\n",
    "In the empty cell below try calling the function `split_tokens` on `sample_text`. What happens?\n",
    "\n",
    "Notice that the programme does not make a simple function call, it uses it in a list comprehension which iterates over it. Another common way to collect the yields would be with a for loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<enumerate at 0x11297f500>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = split_tokens(sample_text)\n",
    "enumerate(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas dataframes\n",
    "We will be using tables in various ways later in the module. We now look at how to store tables as Pandas dataframes. \n",
    "\n",
    "If you want more detais, a good starting point is [10 Minutes to Pandas](https://pandas.pydata.org/pandas-docs/stable/10min.html).\n",
    "\n",
    "First, let's create some data to put in the table. This is meant to be the results of some experiment that we have underaken. \n",
    "\n",
    "To do this we create a list of tuples, where each tuple is a row in the table.\n",
    "- We use `display` rather than `print` as it produces a nicer looking table.\n",
    "\n",
    "Run the cell and make sure you understand the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sample Size  Accuracy\n",
      "0           10     0.674\n",
      "1           20     0.708\n",
      "2           30     0.721\n",
      "3           40     0.744\n",
      "4           50     0.748\n",
      "5           60     0.759\n",
      "6           70     0.762\n",
      "7           80     0.769\n",
      "8           90     0.773\n",
      "9          100     0.775\n"
     ]
    }
   ],
   "source": [
    "results = [\n",
    "    (10,0.674),\n",
    "    (20,0.708),\n",
    "    (30,0.721),\n",
    "    (40,0.744),\n",
    "    (50,0.748),\n",
    "    (60,0.759),\n",
    "    (70,0.762),\n",
    "    (80,0.769),\n",
    "    (90,0.773),\n",
    "    (100,0.775)]\n",
    "df = pd.DataFrame(results,columns = [\"Sample Size\",\"Accuracy\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a table from columns\n",
    "We now create the same dataframe, but in a different way. This time we specify the contents by giving a list for each column.\n",
    "- The column lists and `zip`'d together to create the same list of tuples we saw above, one tuple for each row of the table.\n",
    "- `zip` returns an iterator of tuples, so  `list` is needed to give the required list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10, 0.674), (20, 0.708), (30, 0.721), (40, 0.744), (50, 0.748), (60, 0.759), (70, 0.762), (80, 0.769), (90, 0.773), (100, 0.775)]\n",
      "   Sample Size  Score\n",
      "0           10  0.674\n",
      "1           20  0.708\n",
      "2           30  0.721\n",
      "3           40  0.744\n",
      "4           50  0.748\n",
      "5           60  0.759\n",
      "6           70  0.762\n",
      "7           80  0.769\n",
      "8           90  0.773\n",
      "9          100  0.775\n"
     ]
    }
   ],
   "source": [
    "sample_sizes = list(range(10,110,10))\n",
    "scores = [0.674,0.708,0.721,0.744,0.748,0.759,0.762,0.769,0.773,0.775,0.324234]\n",
    "print zip(sample_sizes,scores)\n",
    "df = pd.DataFrame(list(zip(sample_sizes,scores)),columns = [\"Sample Size\",\"Score\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting data in a dataframe\n",
    "In the following cell we see how to plot the dataframe containing our pretend experimental results.\n",
    "- Note that some of the settings are determined by code in the first cell of the notebook.\n",
    "- `x=0` indicates that the first column of the data provides the values on the x-axis.\n",
    "- See [pandas.DataFrame.plot](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax = df.plot(kind=\"bar\",x=0,legend=False,title=\"Experimental Results\",yticks=(0.6,0.65,0.7,0.75,0.8))\n",
    "# set the x-axis label\n",
    "ax.set_xlabel(\"Sample Size\")\n",
    "# set the y-axis label\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "# set the y axis range \n",
    "ax.set_ylim(0.6,0.8)\n",
    "ax.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have results for two competing methods. \n",
    "\n",
    "We will have a three rather than two columns in our dataframe:\n",
    "- the first column holds the sample size\n",
    "- the second column holds one set of results\n",
    "- the third column holds a second set of results\n",
    "\n",
    "Run the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Sample Size  Your Score  My Score\n",
      "0           10       0.674     0.774\n",
      "1           20       0.708     0.788\n",
      "2           30       0.721     0.801\n",
      "3           40       0.744     0.844\n",
      "4           50       0.748     0.852\n",
      "5           60       0.759     0.855\n",
      "6           70       0.762     0.860\n",
      "7           80       0.769     0.862\n",
      "8           90       0.773     0.863\n",
      "9          100       0.775     0.864\n"
     ]
    }
   ],
   "source": [
    "sample_sizes = list(range(10,110,10))\n",
    "your_results = [0.674,0.708,0.721,0.744,0.748,0.759,0.762,0.769,0.773,0.775]\n",
    "my_results = [0.774,0.788,0.801,0.844,0.852,0.855,0.860,0.862,0.863,0.864]\n",
    "\n",
    "df = pd.DataFrame(list(zip(sample_sizes,your_results,my_results)),columns = [\"Sample Size\",\"Your Score\",\"My Score\"])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we show how to visualise these results.\n",
    "- This time we want a legend.\n",
    "- We also need to expand the limits being shown on the y-axis\n",
    "\n",
    "Run the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6, 0.9)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ax = df.plot(kind=\"bar\",x=0,title=\"Experimental Results\",yticks=(0.6,0.65,0.7,0.75,0.8))\n",
    "# set the x-axis label\n",
    "ax.set_xlabel(\"Sample Size\")\n",
    "# set the y-axis label\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "# set the y axis range \n",
    "ax.set_ylim(0.6,0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a    1.0\n",
      "b    2.0\n",
      "c    3.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11297e6d0>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\"a\": 1., \"b\":2.,\"c\":3.}\n",
    "s = pd.Series(data)\n",
    "print(s)\n",
    "s.plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a python program\n",
    "We now look at the difference between three different ways of running a python program. \n",
    "\n",
    "The first is the way used in the above examples: simply typing or pasting the code into a notebook (or console) and running it.\n",
    "\n",
    "Very similar to the first way is to import the code from a file or module into a notebook (or console). If you import a module, python will automatically run it. That means it reads every line in the file and executes. If the module contains function definitions, executing them means creating the functions. If it contains code that calls functions, python will make those calls and run the functions.  \n",
    "\n",
    "The third way is to run the module from the command line by typing python followed by the module name including the `.py` suffix.\n",
    "\n",
    "Python behaves the same for the second and third method. However, it is often useful to have a module that runs using the third method but doesn't run using the second i.e. you can import the functions, and perhaps some variables, without running anything. To achieve this, modules often include the line  \n",
    "- `if __name__ == \"__main__\"`  \n",
    "as in the cell below. \n",
    "\n",
    "This will run when called from the command line, but not when the file is imported.\n",
    "\n",
    "The cell below contains the programmes for the tokens exercise. It is also stored in a file named \"Exercise.py\" You don't need to read the code as nothing has changed (apart from the addition of one line for testing which was added only to the saved file). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This', 'alpha')\n",
      "('is', 'alpha')\n",
      "('a', 'alpha')\n",
      "('sample', 'alpha')\n",
      "('sentence01', 'alnum')\n",
      "('showing', 'alpha')\n",
      "('7', 'digit')\n",
      "('different', 'alpha')\n",
      "('token', 'alpha')\n",
      "('types', 'alpha')\n",
      "(':', 'punctuation')\n",
      "('alphabetic', 'alpha')\n",
      "(',', 'punctuation')\n",
      "('numeric', 'alpha')\n",
      "(',', 'punctuation')\n",
      "('alphanumeric', 'alpha')\n",
      "(',', 'punctuation')\n",
      "('Title', 'alpha')\n",
      "(',', 'punctuation')\n",
      "('UPPERCASE', 'alpha')\n",
      "(',', 'punctuation')\n",
      "('CamelCase', 'alpha')\n",
      "('and', 'alpha')\n",
      "('punctuation', 'alpha')\n",
      "('!', 'sentence_end')\n",
      "('\\n', 'paragraph_end')\n",
      "('Sentences', 'alpha')\n",
      "('like', 'alpha')\n",
      "('that', 'alpha')\n",
      "('should', 'alpha')\n",
      "('not', 'alpha')\n",
      "('exist', 'alpha')\n",
      "('.', 'sentence_end')\n",
      "(\"They're\", 'other')\n",
      "('too', 'alpha')\n",
      "('artificial', 'alpha')\n",
      "('.', 'sentence_end')\n",
      "('\\n', 'paragraph_end')\n",
      "('A', 'alpha')\n",
      "('REAL', 'alpha')\n",
      "('sentence', 'alpha')\n",
      "('looks', 'alpha')\n",
      "('different', 'alpha')\n",
      "('.', 'sentence_end')\n",
      "('It', 'alpha')\n",
      "('has', 'alpha')\n",
      "('flavour', 'alpha')\n",
      "('to', 'alpha')\n",
      "('it', 'alpha')\n",
      "('.', 'sentence_end')\n",
      "('You', 'alpha')\n",
      "('can', 'alpha')\n",
      "('smell', 'alpha')\n",
      "('it;', 'other')\n",
      "(\"it's\", 'other')\n",
      "('like', 'alpha')\n",
      "('Pythonic', 'alpha')\n",
      "('code', 'alpha')\n",
      "(',', 'punctuation')\n",
      "('you', 'alpha')\n",
      "('know', 'alpha')\n",
      "('?', 'sentence_end')\n",
      "('\\n', 'paragraph_end')\n",
      "('Have', 'alpha')\n",
      "('you', 'alpha')\n",
      "('heard', 'alpha')\n",
      "('of', 'alpha')\n",
      "(\"'code\", 'other')\n",
      "(\"smell'\", 'other')\n",
      "('?', 'sentence_end')\n",
      "('Google', 'alpha')\n",
      "('it', 'alpha')\n",
      "('if', 'alpha')\n",
      "('you', 'alpha')\n",
      "(\"haven't\", 'other')\n",
      "('.', 'sentence_end')\n"
     ]
    }
   ],
   "source": [
    "def make_tokens(input_text):\n",
    "    \"\"\"\n",
    "    Take an input text, split it into tokens, find the\n",
    "    token's shape, make a feature\n",
    "    vector with the token itself and its shape, return\n",
    "    a list of all token feature vectors found in the input.\n",
    "    :param input_text: A character string containing spaces\n",
    "    :return: A list of token feature vectors (token, shape).\n",
    "        Sample output: [('a', 'alpha'), ('7', 'digit'), ('A27', 'alnum')]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Now it's up to the split_tokes function to decide what a token is.\n",
    "    # List comprehension creates a list by extracting elements from\n",
    "    # an iterable object, in this case Python automatically converts the\n",
    "    # split_tokens function into an iterable object because it uses the \"yield\" statement:\n",
    "    \n",
    "    tokens = [token for token in split_tokens(input_text)]\n",
    "    return map(make_token_feature_vector, tokens)\n",
    "\n",
    "\n",
    "def make_token_feature_vector(token):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given a token, extract its shape and return a\n",
    "    vector with the token itself and its shape\n",
    "    :param token: A character string\n",
    "    :return: A tuple (token, shape)\n",
    "    \"\"\"\n",
    "    \n",
    "    if token.isalpha():\n",
    "        return (token, \"alpha\")\n",
    "    elif token.isdigit():\n",
    "        return (token, \"digit\")\n",
    "    elif token.isalnum():\n",
    "        return (token, \"alnum\")\n",
    "    elif token in \",:;\":  \n",
    "        return (token, \"punctuation\")\n",
    "    elif token in \".!?\":  \n",
    "        return (token, \"sentence_end\")\n",
    "    elif token == \"\\n\":  \n",
    "        return (token, \"paragraph_end\")\n",
    "    else:\n",
    "        return (token, \"other\")\n",
    "\n",
    "\n",
    "\n",
    "def split_tokens(input_text):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function decides how to delimit a token. It takes an input\n",
    "    string, iterates over it character by character; it collects\n",
    "    constituent characters in the output token; punctuation characters\n",
    "    are considered delimiters therefore become tokens of their own; the\n",
    "    space character is removed from tokens. Yield each found token at\n",
    "    a time.\n",
    "    :param input_text: A character string containing a mix of text and delimiter characters.\n",
    "    :yield A character string which is either free from delimiters or\n",
    "        is a delimiter itself.\n",
    "    \"\"\"\n",
    "    \n",
    "    # First decide what characters delimit a token:\n",
    "    DELIMITERS = \",:!?.\\n\"\n",
    "    \n",
    "    token = \"\"\n",
    "    for char in input_text:\n",
    "        \n",
    "        if char in DELIMITERS:  # test if the input character is a delimiter (substring presence)\n",
    "            \n",
    "            # Character strings, lists, etc, have a logical truth value in Python;\n",
    "            # an empty string is False, if it has characters it is True.\n",
    "            \n",
    "            if not token:  # same as token == \"\"\n",
    "                yield char\n",
    "            else:\n",
    "                \n",
    "                # Return token to the calling program, but next time this function\n",
    "                # is called, continue from\n",
    "                # the next statement rather than from the beginning of the function:\n",
    "                \n",
    "                yield token  # After yielding control to the calling program,\n",
    "                             # this function will execute the next statement:\n",
    "                token = \"\"  # Pick up execution from here.\n",
    "                yield char\n",
    "        elif char == \" \":\n",
    "            if token:  # same as token != \"\"\n",
    "                yield token\n",
    "                token = \"\"\n",
    "        else:\n",
    "            token += char\n",
    "            \n",
    "sample_text = \"This is a sample sentence01 showing 7 different token types: alphabetic, numeric, alphanumeric, Title, UPPERCASE, CamelCase and punctuation!\\nSentences like that should not exist. They're too artificial.\\nA REAL sentence looks different. It has flavour to it. You can smell it; it's like Pythonic code, you know?\\nHave you heard of 'code smell'? Google it if you haven't.\"            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for token in make_tokens(sample_text):\n",
    "        print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Try the following.\n",
    "\n",
    "1. Execute the cell above and look at what happens.\n",
    "\n",
    "2. In the empty cell below execute:  \n",
    "`import Exercise`  \n",
    "Note the capital letter in the filename. \n",
    "It should not run the programme. \n",
    "\n",
    "To understand what has happened, run each the following commands one at a time:  \n",
    "`print(noone)`  \n",
    "`print Exercise.noone`  \n",
    "`from Exercise import noone`  \n",
    "`print(noone)` \n",
    "\n",
    "The variable `noone` did not exist in the original programme (it was assigned in the test line that was added to the file).\n",
    "- Notice the difference between the two types of import. Using the second type is more convenient as you don't have to specify the namespace to access functions and variables.\n",
    "- For this reason people sometimes use the command  \n",
    "`from module import *`  \n",
    "However, this is dangerous as you can easily overwrite existing names and python will not warn you. Using the import command in this way is considered bad practice. You can sometimes get away with it when importing your own module, but avoid it with library modules.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note on terminology. The word \"parse\" means to read and process sequentially. In NLP it also has a specific meaning to analyse text to determine its syntax. To avoid confusion please be aware that in this exercise the first meaning is used.\n",
    "\n",
    "The programme below constructs a nested list by reading some input text and looking for delimiters.\n",
    "- First it runs our `make_tokens` function.\n",
    "- Then it reads one token at a time to construct sentences. Each sentence is a list.\n",
    "- When the end of a sentence is reached, a new empty list is created and a new sentence read.\n",
    "- When it reaches the end of a paragraph, all the sentences in that paragraph are kept together in a list, and a new pargraph is created.\n",
    "\n",
    "There are various ways this could be done. The method below uses the generator method we have seen before where results are delivered using the yield command, instead of the return command. This means the function does not exit, but resumes from the same place the next time it is called.\n",
    "- Using generators is often a good way to write clear simple code\n",
    "- Another advantages of the generator method is that it enables data to be processed as it is needed, making it possible to process very large lists that might use up too much memory.\n",
    "\n",
    "### Exercise\n",
    "Execute the  cell below and study the code until you understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************** SENTENCES IN THE PARSED TEXT:\n",
      "[('This', 'alpha'), ('is', 'alpha'), ('a', 'alpha'), ('sample', 'alpha'), ('sentence01', 'alnum'), ('showing', 'alpha'), ('7', 'digit'), ('different', 'alpha'), ('token', 'alpha'), ('types', 'alpha'), (':', 'punctuation'), ('alphabetic', 'alpha'), (',', 'punctuation'), ('numeric', 'alpha'), (',', 'punctuation'), ('alphanumeric', 'alpha'), (',', 'punctuation'), ('Title', 'alpha'), (',', 'punctuation'), ('UPPERCASE', 'alpha'), (',', 'punctuation'), ('CamelCase', 'alpha'), ('and', 'alpha'), ('punctuation', 'alpha')]\n",
      "paragraph_end\n",
      "[('Sentences', 'alpha'), ('like', 'alpha'), ('that', 'alpha'), ('should', 'alpha'), ('not', 'alpha'), ('exist', 'alpha')]\n",
      "[(\"They're\", 'other'), ('too', 'alpha'), ('artificial', 'alpha')]\n",
      "paragraph_end\n",
      "[('A', 'alpha'), ('REAL', 'alpha'), ('sentence', 'alpha'), ('looks', 'alpha'), ('different', 'alpha')]\n",
      "[('It', 'alpha'), ('has', 'alpha'), ('flavour', 'alpha'), ('to', 'alpha'), ('it', 'alpha')]\n",
      "[('You', 'alpha'), ('can', 'alpha'), ('smell', 'alpha'), ('it;', 'other'), (\"it's\", 'other'), ('like', 'alpha'), ('Pythonic', 'alpha'), ('code', 'alpha'), (',', 'punctuation'), ('you', 'alpha'), ('know', 'alpha')]\n",
      "paragraph_end\n",
      "[('Have', 'alpha'), ('you', 'alpha'), ('heard', 'alpha'), ('of', 'alpha'), (\"'code\", 'other'), (\"smell'\", 'other')]\n",
      "[('Google', 'alpha'), ('it', 'alpha'), ('if', 'alpha'), ('you', 'alpha'), (\"haven't\", 'other')]\n",
      "************************** PARAGRAPHS IN THE PARSED TEXT:\n",
      "[[('This', 'alpha'), ('is', 'alpha'), ('a', 'alpha'), ('sample', 'alpha'), ('sentence01', 'alnum'), ('showing', 'alpha'), ('7', 'digit'), ('different', 'alpha'), ('token', 'alpha'), ('types', 'alpha'), (':', 'punctuation'), ('alphabetic', 'alpha'), (',', 'punctuation'), ('numeric', 'alpha'), (',', 'punctuation'), ('alphanumeric', 'alpha'), (',', 'punctuation'), ('Title', 'alpha'), (',', 'punctuation'), ('UPPERCASE', 'alpha'), (',', 'punctuation'), ('CamelCase', 'alpha'), ('and', 'alpha'), ('punctuation', 'alpha')]]\n",
      "[[('Sentences', 'alpha'), ('like', 'alpha'), ('that', 'alpha'), ('should', 'alpha'), ('not', 'alpha'), ('exist', 'alpha')], [(\"They're\", 'other'), ('too', 'alpha'), ('artificial', 'alpha')]]\n",
      "[[('A', 'alpha'), ('REAL', 'alpha'), ('sentence', 'alpha'), ('looks', 'alpha'), ('different', 'alpha')], [('It', 'alpha'), ('has', 'alpha'), ('flavour', 'alpha'), ('to', 'alpha'), ('it', 'alpha')], [('You', 'alpha'), ('can', 'alpha'), ('smell', 'alpha'), ('it;', 'other'), (\"it's\", 'other'), ('like', 'alpha'), ('Pythonic', 'alpha'), ('code', 'alpha'), (',', 'punctuation'), ('you', 'alpha'), ('know', 'alpha')]]\n",
      "[[('Have', 'alpha'), ('you', 'alpha'), ('heard', 'alpha'), ('of', 'alpha'), (\"'code\", 'other'), (\"smell'\", 'other')], [('Google', 'alpha'), ('it', 'alpha'), ('if', 'alpha'), ('you', 'alpha'), (\"haven't\", 'other')]]\n",
      "************************** PARSED TEXT:\n",
      "[[[('This', 'alpha'), ('is', 'alpha'), ('a', 'alpha'), ('sample', 'alpha'), ('sentence01', 'alnum'), ('showing', 'alpha'), ('7', 'digit'), ('different', 'alpha'), ('token', 'alpha'), ('types', 'alpha'), (':', 'punctuation'), ('alphabetic', 'alpha'), (',', 'punctuation'), ('numeric', 'alpha'), (',', 'punctuation'), ('alphanumeric', 'alpha'), (',', 'punctuation'), ('Title', 'alpha'), (',', 'punctuation'), ('UPPERCASE', 'alpha'), (',', 'punctuation'), ('CamelCase', 'alpha'), ('and', 'alpha'), ('punctuation', 'alpha')]], [[('Sentences', 'alpha'), ('like', 'alpha'), ('that', 'alpha'), ('should', 'alpha'), ('not', 'alpha'), ('exist', 'alpha')], [(\"They're\", 'other'), ('too', 'alpha'), ('artificial', 'alpha')]], [[('A', 'alpha'), ('REAL', 'alpha'), ('sentence', 'alpha'), ('looks', 'alpha'), ('different', 'alpha')], [('It', 'alpha'), ('has', 'alpha'), ('flavour', 'alpha'), ('to', 'alpha'), ('it', 'alpha')], [('You', 'alpha'), ('can', 'alpha'), ('smell', 'alpha'), ('it;', 'other'), (\"it's\", 'other'), ('like', 'alpha'), ('Pythonic', 'alpha'), ('code', 'alpha'), (',', 'punctuation'), ('you', 'alpha'), ('know', 'alpha')]], [[('Have', 'alpha'), ('you', 'alpha'), ('heard', 'alpha'), ('of', 'alpha'), (\"'code\", 'other'), (\"smell'\", 'other')], [('Google', 'alpha'), ('it', 'alpha'), ('if', 'alpha'), ('you', 'alpha'), (\"haven't\", 'other')]]]\n"
     ]
    }
   ],
   "source": [
    "def parse_text(input_text):\n",
    "    \"\"\"\n",
    "    A parsed text is defined as a list of parsed paragraphs.\n",
    "    Given an input text, parse its paragraphs and return a list\n",
    "    with the results.\n",
    "    :param input_text: A character string with paragraphs\n",
    "    :return: A list of parsed paragraphs\n",
    "    \"\"\"\n",
    "    \n",
    "    return [paragraph for paragraph in parse_paragraphs(input_text)]\n",
    "\n",
    "\n",
    "def parse_paragraphs(input_text):\n",
    "    \"\"\"\n",
    "    A parsed paragraph is defined as a list of parsed sentences.\n",
    "    Given an input text, parse its sentences; if the sentence is\n",
    "    actually the end of a paragraph, then yield the previous\n",
    "    sentences packed as a list.\n",
    "    :param input_text: a character string containing paragraphs\n",
    "                       and sentences.\n",
    "    :yield: A list of sentences up to the end of the paragraph.\n",
    "    \"\"\"\n",
    "    \n",
    "    paragraph = list()\n",
    "    for sentence in parse_sentences(input_text):\n",
    "        \n",
    "        # We expect parse_sentences to return \"paragraph_end\"\n",
    "        # when it encounters an end of paragraph mark.\n",
    "        \n",
    "        if sentence == \"paragraph_end\":\n",
    "            yield paragraph\n",
    "            paragraph = list()\n",
    "        else:\n",
    "            paragraph.append(sentence)\n",
    "    yield paragraph\n",
    "\n",
    "\n",
    "def parse_sentences(input_text):\n",
    "    \"\"\"\n",
    "    A parsed sentence is defined as a list of token vectors\n",
    "    :param input_text: a character string containing paragraphs,\n",
    "                       sentences and token vectors.\n",
    "    :yield: A list of token vectors up to the end of a sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    token_vectors = make_tokens(input_text)  \n",
    "    sentence = list()\n",
    "    \n",
    "    # Since a token vector is a tuple (token, shape) we can unpack it\n",
    "    # automatically as we iterate over the list of token vectors:\n",
    "    \n",
    "    for token, shape in token_vectors:\n",
    "        if shape == \"sentence_end\":\n",
    "            yield sentence\n",
    "            sentence = list()\n",
    "        elif shape == \"paragraph_end\":\n",
    "            if sentence:\n",
    "                yield sentence\n",
    "                sentence = list()\n",
    "            yield \"paragraph_end\"\n",
    "        else:\n",
    "            sentence.append((token, shape))\n",
    "    if sentence:\n",
    "        yield sentence\n",
    "\n",
    "\n",
    "\n",
    "print(\"************************** SENTENCES IN THE PARSED TEXT:\")\n",
    "for sentence in parse_sentences(sample_text):\n",
    "    print(sentence)\n",
    "print(\"************************** PARAGRAPHS IN THE PARSED TEXT:\")\n",
    "\n",
    "for paragraph in parse_paragraphs(sample_text):\n",
    "    print(paragraph)\n",
    "\n",
    "print(\"************************** PARSED TEXT:\")\n",
    "print(parse_text(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "The programme in the cell below selects a character at random from the nested list generated by the previous programme. Run the cell and make sure you can understand what it is doing.\n",
    "\n",
    "1. Recall that we defined a token vector to be an ordered pair (token, shape). Accessing the token or the shape with the code `token_vector[0]` or `token_vector[1]` is difficult to read. It is better to define the indices as constants. Constants are always given capitalised names and sit in the global scope. Do you agree that this improves readability?\n",
    "\n",
    "2. Notice how to index into the nested list and the character string. The line indexing the character string could have been written as:  \n",
    "\n",
    "`character = parsed_text[paragraph_coord][sentence_coord][token_coord][TOKEN][character_coord]`\n",
    "\n",
    "Do you think this would have made the programme more readable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I', 2, 1, 0, 0)\n",
      "('e', 1, 0, 1, 3)\n",
      "('w', 0, 0, 5, 3)\n",
      "('t', 1, 1, 1, 0)\n",
      "(',', 0, 0, 14, 0)\n",
      "('s', 1, 0, 3, 0)\n",
      "('i', 1, 1, 2, 3)\n",
      "('a', 2, 1, 1, 1)\n",
      "(',', 0, 0, 16, 0)\n",
      "('n', 0, 0, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "TOKEN = 0  \n",
    "SHAPE = 1\n",
    "\n",
    "def get_random_character_coordinates_in_text(parsed_text):\n",
    "    \"\"\"\n",
    "    Given a parsed text, as the one produced by parse_text.py,\n",
    "    return a random character within the text, together with its\n",
    "    coordinates.\n",
    "    :param parsed_text: A nested list with token vectors within\n",
    "        sentence lists within paragraph lists.\n",
    "    :return: A vector where the elements are: the random character,\n",
    "        the paragraph, sentence, token and character coordinates.\n",
    "        Sample output: ('f', 3, 1, 2, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate a random index within a valid range:\n",
    "    \n",
    "    paragraph_coord = random.randrange(len(parsed_text))\n",
    "    sentence_coord = random.randrange(len(parsed_text[paragraph_coord]))\n",
    "    token_coord = random.randrange(len(parsed_text[paragraph_coord][sentence_coord]))\n",
    "    token = parsed_text[paragraph_coord][sentence_coord][token_coord][TOKEN]\n",
    "    character_coord = random.randrange(len(token))\n",
    "        \n",
    "    \n",
    "    # With the obtained random coordinates, access the input parsed text:\n",
    "    \n",
    "    character = token[character_coord]\n",
    "    \n",
    "    return character, paragraph_coord, sentence_coord, token_coord, character_coord\n",
    "\n",
    "\n",
    "parsed_text = parse_text(sample_text)\n",
    "for _ in range(10):\n",
    "    print(get_random_character_coordinates_in_text(parsed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
